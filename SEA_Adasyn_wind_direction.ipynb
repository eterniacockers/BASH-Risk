{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eterniacockers/BASH-Risk/blob/main/SEA_Adasyn_wind_direction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r4PItpAxf4p"
      },
      "source": [
        "Start by uploading the imputed data file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHwJ_20Kqtah"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read Excel file\n",
        "file_name = list(uploaded.keys())[0]  # Get uploaded filename\n",
        "df = pd.read_excel(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1.1 Preprocessing: Initial cleaning and encoding\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel(\"SEA_Data_Python_Imputed.xlsx\")\n",
        "\n",
        "# Convert date column to datetime and sort by date\n",
        "df['DATE_and_Time'] = pd.to_datetime(df['DATE_and_Time'])\n",
        "df.sort_values(by='DATE_and_Time', inplace=True)\n",
        "\n",
        "# Quartile transformation for Max_Risk (Used ONLY for XGBoost)\n",
        "df['Max_Risk_Quartile'] = pd.qcut(df['Max_Risk'], q=4, labels=False)\n",
        "\n",
        "# Normalize numerical features BEFORE lagging\n",
        "numeric_vars = [\"Wet_Bulb\", \"Wind_Speed\", \"Altimeter\", \"Wind_Direction\"]\n",
        "scaler = MinMaxScaler()\n",
        "df[numeric_vars] = scaler.fit_transform(df[numeric_vars])\n",
        "\n",
        "# One-hot encode Risk (Only needed for lagging step, will remove later)\n",
        "df = pd.get_dummies(df, columns=[\"Risk\"], drop_first=False)\n",
        "\n",
        "# Create a single categorical target column for LSTM (0 = Low, 1 = Moderate, 2 = Severe)\n",
        "risk_mapping = {\"Low\": 0, \"Moderate\": 1, \"Severe\": 2}\n",
        "df['Risk_Category'] = df[['Risk_Low', 'Risk_Moderate', 'Risk_Severe']].idxmax(axis=1).map(lambda x: risk_mapping[x.replace(\"Risk_\", \"\")])\n",
        "\n",
        "# Drop unnecessary columns (Exclude `Max_Risk_Quartile` for LSTM, Exclude `Risk_Category` and one-hot Risk for XGBoost)\n",
        "df.drop(columns=[\"Station\", \"4_hr_Precip\", \"4_hr_Sky_Cond\",\n",
        "                 \"Visibility\", \"Sky_Cond\",\n",
        "                 \"10_Day_Precip\", \"Humidity\", \"Precipitation\", \"Max_Risk\"], inplace=True)\n",
        "\n",
        "# Display verification\n",
        "print(\"Step 1.1 Completed. Sample rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Save preprocessed data for verification before feature engineering\n",
        "df.to_csv(\"SEA_Preprocessed_2.csv\", index=False)"
      ],
      "metadata": {
        "id": "6khKG7K6EwWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhM7i5c4xlDB"
      },
      "source": [
        "Encode data, remove excess features and prepare for the different ensemble models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XX9A8YVOs_H6",
        "outputId": "5acdcc91-2310-4a0f-9142-7aba3a4bcfd3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_64786d40-5e77-4ad3-b2a1-5f3ab1fb3a40\", \"SEA_Preprocessed_2.csv\", 3044890)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# download file\n",
        "files.download(\"SEA_Preprocessed_2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrsMAe31sKue",
        "outputId": "78b3ec3c-aee0-43f7-8e14-4863710f1e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification of Risk Levels:\n",
            "Risk_Low: 21574 instances\n",
            "Risk_Moderate: 4593 instances\n",
            "Risk_Severe: 436 instances\n"
          ]
        }
      ],
      "source": [
        "# Step 1.2 Check Risk Levels\n",
        "# Ensure Risk_Low is correctly calculated\n",
        "if 'Risk_Low' not in df.columns:\n",
        "    df['Risk_Low'] = 1 - (df['Risk_Moderate'] + df['Risk_Severe'])\n",
        "\n",
        "# Check the counts of each Risk level\n",
        "risk_summary = {\n",
        "    'Risk_Low': (df['Risk_Low'] == 1).sum(),\n",
        "    'Risk_Moderate': (df['Risk_Moderate'] == 1).sum(),\n",
        "    'Risk_Severe': (df['Risk_Severe'] == 1).sum()\n",
        "}\n",
        "\n",
        "# Print the counts for verification\n",
        "print(\"Verification of Risk Levels:\")\n",
        "for risk_level, count in risk_summary.items():\n",
        "    print(f\"{risk_level}: {count} instances\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvsdW9PdsSd7"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 1.3 - Preprocessing: Add lags, cyclical encoding, and create LSTM/XGBoost datasets (Colab version)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# ‚úÖ Load dataset (Assumes the file has been uploaded to Colab)\n",
        "df = pd.read_csv(\"SEA_Preprocessed_2.csv\")\n",
        "\n",
        "# Ensure DATE_and_Time is properly converted to datetime\n",
        "df['DATE_and_Time'] = pd.to_datetime(df['DATE_and_Time'], errors='coerce')\n",
        "\n",
        "# ‚úÖ Drop any existing lagged columns to avoid duplication\n",
        "lagged_features_to_check = [\n",
        "    \"Max_Risk_Quartile_Lag_2\", \"Max_Risk_Quartile_Lag_24\",\n",
        "    \"Wet_Bulb_Lag_2\", \"Wet_Bulb_Lag_3\", \"Wind_Speed_Lag_2\",\n",
        "    \"Risk_Low_Lag_2\", \"Risk_Low_Lag_24\",\n",
        "    \"Risk_Moderate_Lag_2\", \"Risk_Moderate_Lag_24\",\n",
        "    \"Risk_Severe_Lag_2\", \"Risk_Severe_Lag_24\",\n",
        "    \"Altimeter_Lag_2\", \"Wind_Direction_Lag_2\"  # ‚úÖ Removing Altimeter_Lag_2 due to redundancy\n",
        "]\n",
        "\n",
        "df.drop(columns=[col for col in lagged_features_to_check if col in df.columns], inplace=True, errors=\"ignore\")\n",
        "\n",
        "# ‚úÖ Define feature groups\n",
        "numerical_features = [\"Max_Risk_Quartile\", \"Wet_Bulb\", \"Wind_Speed\", \"Altimeter\", \"Wind_Direction\"]\n",
        "categorical_features = [\"Risk_Low\", \"Risk_Moderate\", \"Risk_Severe\"]\n",
        "\n",
        "# ‚úÖ Dictionary to store lagged columns\n",
        "lagged_columns = {}\n",
        "\n",
        "# ‚úÖ Generate lags for numerical features\n",
        "for col in numerical_features:\n",
        "    if col in df.columns:\n",
        "        lags = [2, 24] if col == \"Max_Risk_Quartile\" else [3] if col == \"Altimeter\" else [2, 3] if col == \"Wet_Bulb\" else [2]\n",
        "        for lag in lags:\n",
        "            lagged_columns[f\"{col}_Lag_{lag}\"] = df[col].shift(lag)\n",
        "\n",
        "# ‚úÖ Generate lags for categorical features\n",
        "for col in categorical_features:\n",
        "    if col in df.columns:\n",
        "        for lag in [2, 24]:  # Apply only to meaningful time shifts\n",
        "            lagged_columns[f\"{col}_Lag_{lag}\"] = df[col].shift(lag)\n",
        "\n",
        "# ‚úÖ Create a DataFrame for lagged features\n",
        "lagged_df = pd.DataFrame(lagged_columns, index=df.index)\n",
        "\n",
        "# ‚úÖ Merge lagged features with the original dataset\n",
        "df = pd.concat([df, lagged_df], axis=1)\n",
        "\n",
        "# ‚úÖ Drop rows with NaN values introduced by lagging\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# ‚úÖ Cyclical encoding for hour and month\n",
        "df['hour'] = df['DATE_and_Time'].dt.hour\n",
        "df['month'] = df['DATE_and_Time'].dt.month\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "# ‚úÖ Create separate datasets for LSTM and XGBoost\n",
        "df_lstm = df.copy()\n",
        "df_xgb = df.copy()\n",
        "\n",
        "# ‚úÖ LSTM: Drop `Max_Risk_Quartile` (regression target) but keep `Risk_Category`\n",
        "df_lstm.drop(columns=['Max_Risk_Quartile', 'Max_Risk_Quartile_Lag_2', 'Max_Risk_Quartile_Lag_24',\n",
        "                      'Risk_Low', 'Risk_Moderate', 'Risk_Severe'], inplace=True, errors='ignore')\n",
        "\n",
        "# ‚úÖ XGBoost: Drop `Risk_Category` and one-hot encoded `Risk` features\n",
        "df_xgb.drop(columns=['Risk_Category', 'Risk_Low', 'Risk_Moderate', 'Risk_Severe',\n",
        "                     'Risk_Low_Lag_2', 'Risk_Low_Lag_24',\n",
        "                     'Risk_Moderate_Lag_2', 'Risk_Moderate_Lag_24',\n",
        "                     'Risk_Severe_Lag_2', 'Risk_Severe_Lag_24'], inplace=True, errors='ignore')\n",
        "\n",
        "# ‚úÖ Drop redundant columns\n",
        "df_lstm.drop(columns=['hour', 'month', 'Altimeter', 'Max_Risk', 'Wind_Speed', 'Wet_Bulb', 'Wind_Direction'], inplace=True, errors=\"ignore\")\n",
        "df_xgb.drop(columns=['hour', 'month', 'Altimeter', 'Max_Risk', 'Wind_Speed', 'Wet_Bulb', 'Wind_Direction'], inplace=True, errors=\"ignore\")\n",
        "\n",
        "# ‚úÖ Save cleaned datasets locally in Colab\n",
        "lstm_filename = \"SEA_LSTM_Preprocessed.csv\"\n",
        "xgb_filename = \"SEA_XGBoost_Preprocessed.csv\"\n",
        "\n",
        "df_lstm.to_csv(lstm_filename, index=False)\n",
        "df_xgb.to_csv(xgb_filename, index=False)\n",
        "\n",
        "print(\"‚úÖ Step 1.3 Completed: Files saved in Colab.\")\n",
        "\n",
        "# ‚úÖ Provide download links for manual saving\n",
        "files.download(lstm_filename)\n",
        "files.download(xgb_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYVpO9xAuRqK"
      },
      "outputs": [],
      "source": [
        "#Step 1.4.1 load data files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtPdnpkEt5-T"
      },
      "outputs": [],
      "source": [
        "#Step 1.4.2 Data file verification\n",
        "import pandas as pd\n",
        "\n",
        "# Load the files (Ensure they exist in the Colab environment)\n",
        "df_lstm = pd.read_csv(\"SEA_LSTM_Preprocessed.csv\")\n",
        "df_xgb = pd.read_csv(\"SEA_XGBoost_Preprocessed.csv\")\n",
        "\n",
        "# Display sample rows\n",
        "print(\"üìå LSTM Preprocessed Dataset (First 5 Rows):\")\n",
        "display(df_lstm.head())\n",
        "\n",
        "print(\"üìå XGBoost Preprocessed Dataset (First 5 Rows):\")\n",
        "display(df_xgb.head())\n",
        "\n",
        "# Check column names to verify preprocessing\n",
        "print(\"üîç LSTM Columns:\", df_lstm.columns.tolist())\n",
        "print(\"üîç XGBoost Columns:\", df_xgb.columns.tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PZTKITDxvax"
      },
      "source": [
        "Split data into different training and validation sets based on time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3swbDxg7uG9o",
        "outputId": "e5722f2b-94cd-4182-c0e9-798f6049d2d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LSTM - X_train shape: (17723, 96, 15), y_train shape: (17723, 3)\n",
            "‚úÖ LSTM - X_val shape: (8664, 96, 15), y_val shape: (8664, 3)\n",
            "‚úÖ XGBoost - Train shape: (17819, 12), Validation shape: (8760, 12)\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Step 2: Split into Training and Validation Sets (Colab Version)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ‚úÖ Load the preprocessed datasets\n",
        "lstm_file_path = \"SEA_LSTM_Preprocessed.csv\"\n",
        "xgb_file_path = \"SEA_XGBoost_Preprocessed.csv\"\n",
        "\n",
        "df_lstm = pd.read_csv(lstm_file_path)\n",
        "df_xgb = pd.read_csv(xgb_file_path)\n",
        "\n",
        "# ‚úÖ Convert DATE_and_Time to datetime (if not already)\n",
        "df_lstm['DATE_and_Time'] = pd.to_datetime(df_lstm['DATE_and_Time'], errors='coerce')\n",
        "df_xgb['DATE_and_Time'] = pd.to_datetime(df_xgb['DATE_and_Time'], errors='coerce')\n",
        "\n",
        "# ‚úÖ Ensure Risk_Category exists in LSTM dataset\n",
        "if 'Risk_Category' not in df_lstm.columns:\n",
        "    raise ValueError(\"‚ùå Risk_Category column is missing in LSTM dataset. Check preprocessing steps.\")\n",
        "\n",
        "# ‚úÖ Ensure Max_Risk_Quartile exists in XGBoost dataset\n",
        "if 'Max_Risk_Quartile' not in df_xgb.columns:\n",
        "    raise ValueError(\"‚ùå Max_Risk_Quartile column is missing in XGBoost dataset. Check preprocessing steps.\")\n",
        "\n",
        "# ‚úÖ Split datasets into training (2015‚Äì2018) and validation (2019)\n",
        "train_lstm = df_lstm[df_lstm['DATE_and_Time'].dt.year < 2019].reset_index(drop=True)\n",
        "val_lstm = df_lstm[df_lstm['DATE_and_Time'].dt.year == 2019].reset_index(drop=True)\n",
        "\n",
        "train_xgb = df_xgb[df_xgb['DATE_and_Time'].dt.year < 2019].reset_index(drop=True)\n",
        "val_xgb = df_xgb[df_xgb['DATE_and_Time'].dt.year == 2019].reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ Drop DATE_and_Time column after splitting\n",
        "train_lstm.drop(columns=['DATE_and_Time'], inplace=True)\n",
        "val_lstm.drop(columns=['DATE_and_Time'], inplace=True)\n",
        "\n",
        "train_xgb.drop(columns=['DATE_and_Time'], inplace=True)\n",
        "val_xgb.drop(columns=['DATE_and_Time'], inplace=True)\n",
        "\n",
        "# ‚úÖ Define function to create sequences for LSTM\n",
        "def create_sequences(data, target, sequence_length=96):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        x = data.iloc[i:(i + sequence_length)].values  # Feature sequence\n",
        "        y = target.iloc[i + sequence_length]  # Corresponding target\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# ‚úÖ Define sequence length for LSTM (96 time steps = 4 days)\n",
        "sequence_length = 96\n",
        "\n",
        "# ‚úÖ Convert Risk_Category to integer before encoding for LSTM\n",
        "train_lstm['Risk_Category'] = train_lstm['Risk_Category'].astype(int)\n",
        "val_lstm['Risk_Category'] = val_lstm['Risk_Category'].astype(int)\n",
        "\n",
        "# ‚úÖ Separate features and target\n",
        "features_train_lstm = train_lstm.drop(columns=['Risk_Category'])\n",
        "target_train_lstm = train_lstm['Risk_Category']\n",
        "features_val_lstm = val_lstm.drop(columns=['Risk_Category'])\n",
        "target_val_lstm = val_lstm['Risk_Category']\n",
        "\n",
        "# ‚úÖ Create sequences for LSTM training and validation\n",
        "X_train_lstm, y_train_lstm = create_sequences(features_train_lstm, target_train_lstm, sequence_length)\n",
        "X_val_lstm, y_val_lstm = create_sequences(features_val_lstm, target_val_lstm, sequence_length)\n",
        "\n",
        "# ‚úÖ One-hot encode target variables for LSTM classification\n",
        "num_classes = 3  # Explicitly set to 3 classes (Low, Moderate, Severe)\n",
        "y_train_lstm = to_categorical(y_train_lstm, num_classes=num_classes)\n",
        "y_val_lstm = to_categorical(y_val_lstm, num_classes=num_classes)\n",
        "\n",
        "# ‚úÖ Save processed LSTM data in Colab\n",
        "np.save(\"X_train_LSTM.npy\", X_train_lstm)\n",
        "np.save(\"y_train_LSTM.npy\", y_train_lstm)\n",
        "np.save(\"X_val_LSTM.npy\", X_val_lstm)\n",
        "np.save(\"y_val_LSTM.npy\", y_val_lstm)\n",
        "\n",
        "# ‚úÖ Save processed XGBoost data in Colab\n",
        "train_xgb.to_csv(\"train_XGBoost.csv\", index=False)\n",
        "val_xgb.to_csv(\"val_XGBoost.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Print shapes for verification\n",
        "print(f\"‚úÖ LSTM - X_train shape: {X_train_lstm.shape}, y_train shape: {y_train_lstm.shape}\")\n",
        "print(f\"‚úÖ LSTM - X_val shape: {X_val_lstm.shape}, y_val shape: {y_val_lstm.shape}\")\n",
        "print(f\"‚úÖ XGBoost - Train shape: {train_xgb.shape}, Validation shape: {val_xgb.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONOqGDRcuWgR",
        "outputId": "020ea4dd-e0ce-4e63-c150-d7f50e7c2826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Correct LSTM Feature Order Extracted & Saved:\n",
            "['Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2', 'Risk_Low_Lag_2', 'Risk_Low_Lag_24', 'Risk_Moderate_Lag_2', 'Risk_Moderate_Lag_24', 'Risk_Severe_Lag_2', 'Risk_Severe_Lag_24', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n"
          ]
        }
      ],
      "source": [
        "#Step 2.1: Feature list for LSTM Feature permutation\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Extract feature order from the original DataFrame\n",
        "lstm_feature_order = features_train_lstm.columns.tolist()\n",
        "\n",
        "# ‚úÖ Save the correct feature order for use in feature importance calculations\n",
        "feature_order_file = \"LSTM_Feature_Order.txt\"\n",
        "with open(feature_order_file, \"w\") as f:\n",
        "    for feature in lstm_feature_order:\n",
        "        f.write(feature + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ Correct LSTM Feature Order Extracted & Saved:\")\n",
        "print(lstm_feature_order)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PB9wP4Hwk2L"
      },
      "outputs": [],
      "source": [
        "#Step 2.2: Save files from data split\n",
        "from google.colab import files\n",
        "\n",
        "# download file\n",
        "files.download(\"X_train_LSTM.npy\")\n",
        "files.download(\"y_train_LSTM.npy\")\n",
        "files.download(\"X_val_LSTM.npy\")\n",
        "files.download(\"y_val_LSTM.npy\")\n",
        "files.download(\"train_XGBoost.csv\")\n",
        "files.download(\"val_XGBoost.csv\")\n",
        "\n",
        "print(f\"‚úÖ Files downloaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-m1ryFrx8Xh"
      },
      "source": [
        "Oversample for the LSTM classification algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FmympjExu4i"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 3: Adasyn for Target Balancing in LSTM (Colab Version)\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import collections\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ‚úÖ Load processed training data\n",
        "X_train = np.load(\"X_train_LSTM.npy\", allow_pickle=True)\n",
        "y_train = np.load(\"y_train_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# ‚úÖ Flatten training features for ADASYN (Convert 3D ‚Üí 2D)\n",
        "train_features = X_train.reshape(X_train.shape[0], -1)\n",
        "train_target = np.argmax(y_train, axis=1)\n",
        "\n",
        "# ‚úÖ Check class distribution before ADASYN\n",
        "class_counts_before = collections.Counter(train_target)\n",
        "print(\"üìä Class distribution before ADASYN:\", class_counts_before)\n",
        "\n",
        "# ‚úÖ Plot class distribution before ADASYN\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=train_target, palette=\"viridis\", hue=train_target, dodge=False)\n",
        "plt.title(\"Class Distribution Before ADASYN\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(ticks=[0, 1, 2], labels=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "plt.legend([], [], frameon=False)\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Apply ADASYN to oversample minority classes\n",
        "adasyn = ADASYN(sampling_strategy='auto', random_state=42, n_neighbors=5)\n",
        "train_features_resampled, train_target_resampled = adasyn.fit_resample(train_features, train_target)\n",
        "\n",
        "# ‚úÖ Check class distribution after ADASYN\n",
        "class_counts_after = collections.Counter(train_target_resampled)\n",
        "print(\"üìä Class distribution after ADASYN:\", class_counts_after)\n",
        "\n",
        "# ‚úÖ Plot class distribution after ADASYN\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=train_target_resampled, palette=\"viridis\", hue=train_target_resampled, dodge=False)\n",
        "plt.title(\"Class Distribution After ADASYN\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(ticks=[0, 1, 2], labels=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "plt.legend([], [], frameon=False)\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Reshape back to 3D for LSTM\n",
        "sequence_length = X_train.shape[1]\n",
        "num_features = X_train.shape[2]\n",
        "train_features_resampled = train_features_resampled.reshape(-1, sequence_length, num_features)\n",
        "\n",
        "# ‚úÖ One-hot encode the resampled targets\n",
        "train_target_resampled = to_categorical(train_target_resampled, num_classes=3)\n",
        "\n",
        "# ‚úÖ Save resampled data\n",
        "save_path = \"resampled_data_adasyn.npz\"\n",
        "np.savez(save_path, X_train=train_features_resampled, y_train=train_target_resampled)\n",
        "print(f\"üìå Resampled data saved at: {save_path}\")\n",
        "\n",
        "# ‚úÖ Download file in Colab\n",
        "from google.colab import files\n",
        "files.download(save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-s7MZkgD_5K",
        "outputId": "2b87e536-7dff-474f-9118-f68da53a2fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_resampled_adasyn dtype: float64\n",
            "y_train_resampled_adasyn dtype: float64\n",
            "X_train_resampled_adasyn shape: (42065, 96, 15)\n",
            "y_train_resampled_adasyn shape: (42065, 3)\n"
          ]
        }
      ],
      "source": [
        "#Step 3.1 print data types and check format: print(\"X_train_resampled dtype:\", X_train_resampled.dtype)\n",
        "# ‚úÖ Load from the saved ADASYN file if needed\n",
        "# data = np.load(\"resampled_data_adasyn.npz\")\n",
        "# X_train_resampled_adasyn = data[\"X_train\"]\n",
        "# y_train_resampled_adasyn = data[\"y_train\"]\n",
        "\n",
        "# ‚úÖ Print shapes and data types\n",
        "print(\"X_train_resampled_adasyn dtype:\", train_features_resampled.dtype)\n",
        "print(\"y_train_resampled_adasyn dtype:\", train_target_resampled.dtype)\n",
        "\n",
        "print(\"X_train_resampled_adasyn shape:\", train_features_resampled.shape)\n",
        "print(\"y_train_resampled_adasyn shape:\", train_target_resampled.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Rzrm3e0BFkNg",
        "outputId": "d0f7aeab-cb30-47e9-bf8e-998d2d9d153f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_target_resampled dtype after conversion: float32\n",
            "y_val_lstm dtype after conversion: float32\n"
          ]
        }
      ],
      "source": [
        "# Step 3.2 Verify correctly saved data\n",
        "# ‚úÖ Convert the labels to float32\n",
        "train_target_resampled = train_target_resampled.astype(np.float32)\n",
        "y_val_lstm = y_val_lstm.astype(np.float32)  # Assuming this is your validation label array\n",
        "\n",
        "# ‚úÖ Check the data types again\n",
        "print(\"train_target_resampled dtype after conversion:\", train_target_resampled.dtype)\n",
        "print(\"y_val_lstm dtype after conversion:\", y_val_lstm.dtype)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8pUXfufyA2Y"
      },
      "source": [
        "Create optimized LSTM with three layers, including a bi-layer, batch normalization, softmax, adam optimizer for learning rate to prevent over training, and early stopping to reduce over training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmgND-NXcZvn",
        "outputId": "462a3da2-ae8b-4a96-c5d9-8bb59e2397ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_resampled dtype: float64\n",
            "y_train_resampled dtype: float32\n",
            "Type of X_train_resampled: <class 'numpy.ndarray'>\n",
            "Type of y_train_resampled: <class 'numpy.ndarray'>\n",
            "X_val_lstm dtype: object\n",
            "y_val_lstm dtype: float32\n"
          ]
        }
      ],
      "source": [
        "# Step 3.3 Check the type and dtype of X_train_resampled and y_train_resampled again\n",
        "# ‚úÖ Aliasing ADASYN-resampled data to existing variable names\n",
        "X_train_resampled = train_features_resampled\n",
        "y_train_resampled = train_target_resampled\n",
        "\n",
        "# ‚úÖ Check dtype\n",
        "print(\"X_train_resampled dtype:\", X_train_resampled.dtype)\n",
        "print(\"y_train_resampled dtype:\", y_train_resampled.dtype)\n",
        "\n",
        "# ‚úÖ Confirm type\n",
        "print(\"Type of X_train_resampled:\", type(X_train_resampled))\n",
        "print(\"Type of y_train_resampled:\", type(y_train_resampled))\n",
        "\n",
        "# ‚úÖ Check validation set dtypes\n",
        "print(\"X_val_lstm dtype:\", X_val_lstm.dtype)\n",
        "print(\"y_val_lstm dtype:\", y_val_lstm.dtype)\n",
        "\n",
        "# ‚úÖ Ensure all arrays are float32 for consistency\n",
        "X_train_resampled = np.array(X_train_resampled, dtype=np.float32)\n",
        "y_train_resampled = np.array(y_train_resampled, dtype=np.float32)\n",
        "X_val_lstm = np.array(X_val_lstm, dtype=np.float32)\n",
        "y_val_lstm = np.array(y_val_lstm, dtype=np.float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyZLzt4KzhIH"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 4.1: Optimized Bi-LSTM Model Training for Classification (Colab Version)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Bidirectional, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "# ‚úÖ Load resampled training data\n",
        "resampled_data = np.load(\"resampled_data_adasyn.npz\")\n",
        "X_train_resampled = resampled_data['X_train']\n",
        "y_train_resampled = resampled_data['y_train']\n",
        "\n",
        "# ‚úÖ Load validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# Ensure all data is in numpy array format and float32\n",
        "X_train_resampled = np.array(X_train_resampled, dtype=np.float32)\n",
        "y_train_resampled = np.array(y_train_resampled, dtype=np.float32)\n",
        "X_val_lstm = np.array(X_val_lstm, dtype=np.float32)\n",
        "y_val_lstm = np.array(y_val_lstm, dtype=np.float32)\n",
        "\n",
        "# Define sequence length and number of features based on X_train_resampled shape\n",
        "sequence_length, num_features = X_train_resampled.shape[1], X_train_resampled.shape[2]\n",
        "\n",
        "# ‚úÖ Define the optimized Bi-LSTM model with explicit sequence_length and num_features\n",
        "model = Sequential([\n",
        "    Input(shape=(sequence_length, num_features)),  # Use the correct shape\n",
        "\n",
        "    # First Bi-LSTM Layer\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Second Bi-LSTM Layer\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Third LSTM Layer (without return_sequences)\n",
        "    LSTM(32),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Fully Connected Dense Layer\n",
        "    Dense(32, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Output Layer for 3-class classification\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# ‚úÖ Compile the model with Adam optimizer and gradient clipping\n",
        "optimizer = Adam(learning_rate=0.0005, clipvalue=1.0)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ‚úÖ Define callbacks for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
        "\n",
        "# ‚úÖ Train the model\n",
        "history = model.fit(\n",
        "    X_train_resampled, y_train_resampled,\n",
        "    epochs=100, batch_size=32,  # Increased batch size for efficiency\n",
        "    validation_data=(X_val_lstm, y_val_lstm),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# ‚úÖ Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_val_lstm, y_val_lstm)\n",
        "print(f\"‚úÖ Validation Loss: {loss:.4f}\")\n",
        "print(f\"‚úÖ Validation Accuracy: {accuracy:.4%}\")\n",
        "\n",
        "# ‚úÖ Save trained model in Colab\n",
        "lstm_model_path = \"LSTM_Trained_Model.keras\"\n",
        "model.save(lstm_model_path)\n",
        "print(f\"üìå Model saved in Colab at: {lstm_model_path}\")\n",
        "\n",
        "# ‚úÖ Save training history\n",
        "history_save_path = \"LSTM_training_history.pkl\"\n",
        "with open(history_save_path, 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "print(f\"üìå Training history saved at: {history_save_path}\")\n",
        "\n",
        "# ‚úÖ Provide download links for saving\n",
        "files.download(lstm_model_path)\n",
        "files.download(history_save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgOj7lCFyZc7"
      },
      "source": [
        "Create XGBoost regression model using the continuous version of Risk named Max_Risk that was quartile transformed. Validate using data from 2019.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI1unFQd02VA"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 4.2: XGBoost Regression Model Training (Colab Version)\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import joblib  # For saving the model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# ‚úÖ Load processed XGBoost data\n",
        "train_xgb = pd.read_csv(\"train_XGBoost.csv\")\n",
        "val_xgb = pd.read_csv(\"val_XGBoost.csv\")\n",
        "\n",
        "# ‚úÖ Separate features and target\n",
        "X_train = train_xgb.drop(columns=['Max_Risk_Quartile'])\n",
        "y_train = train_xgb['Max_Risk_Quartile']\n",
        "\n",
        "X_val = val_xgb.drop(columns=['Max_Risk_Quartile'])\n",
        "y_val = val_xgb['Max_Risk_Quartile']\n",
        "\n",
        "# ‚úÖ Define the optimized XGBoost model\n",
        "xgb_tuned = XGBRegressor(\n",
        "    n_estimators=1000,   # More boosting iterations\n",
        "    learning_rate=0.01,  # Small learning rate for smooth training\n",
        "    max_depth=6,         # Optimized tree depth\n",
        "    colsample_bytree=0.7, # Column sampling to prevent overfitting\n",
        "    reg_alpha=0.1,       # L1 regularization (reduces complexity)\n",
        "    reg_lambda=2,        # L2 regularization (reduces variance)\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ‚úÖ Train the model using the quartile-transformed target variable\n",
        "xgb_tuned.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ Predict on the validation set\n",
        "y_pred = xgb_tuned.predict(X_val)\n",
        "\n",
        "# ‚úÖ Evaluate the model using quartile-transformed predictions\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "# ‚úÖ Print evaluation metrics\n",
        "print(f'‚úÖ Mean Squared Error (Quartile Transform): {mse:.4f}')\n",
        "print(f'‚úÖ R-squared (Quartile Transform): {r2:.4f}')\n",
        "\n",
        "# ‚úÖ Save the trained model in Colab\n",
        "xgb_model_path = \"xgb_tuned_quartile_model.pkl\"\n",
        "joblib.dump(xgb_tuned, xgb_model_path)\n",
        "print(f\"üìå Model saved in Colab at: {xgb_model_path}\")\n",
        "\n",
        "# ‚úÖ Provide download link for saving\n",
        "files.download(xgb_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzILtB3WLdfW",
        "outputId": "ffa245cb-03a7-4fe9-b2ba-f3a58ae81e16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names in X_train: ['Max_Risk_Quartile_Lag_2', 'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Retrieve feature names before converting to NumPy\n",
        "feature_names = train_xgb.drop(columns=['Max_Risk_Quartile']).columns\n",
        "\n",
        "# ‚úÖ Print to ensure target is removed\n",
        "print(\"Feature Names in X_train:\", feature_names.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WeaEuMMsQ5v"
      },
      "outputs": [],
      "source": [
        "#Step 4.3 load data files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmGY3ui79lCF"
      },
      "source": [
        "KNN quartile transformation of the XGBoost regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kev6I8_M9akM"
      },
      "outputs": [],
      "source": [
        "#Up load files\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbq4ZKfP9vsm",
        "outputId": "df4e8439-fd21-401f-a64d-ee3e14c3691c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extracted Max_Risk Data Sample:\n",
            "         DATE_and_Time   Max_Risk\n",
            "0 2016-12-15 16:00:00   33.74463\n",
            "1 2016-12-15 17:00:00   45.50400\n",
            "2 2016-12-15 18:00:00  109.17150\n",
            "3 2016-12-15 19:00:00   15.78886\n",
            "4 2016-12-15 20:00:00   38.86192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-cd5afd3b766b>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_max_risk[\"DATE_and_Time\"] = pd.to_datetime(df_max_risk[\"DATE_and_Time\"])\n"
          ]
        }
      ],
      "source": [
        "#Step 4.4.1 KNN transformation create max_risk raw data file\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load the original file\n",
        "df_raw = pd.read_excel(\"SEA_Data_Python_Imputed.xlsx\")\n",
        "\n",
        "# ‚úÖ Keep only Date/Time and Max_Risk\n",
        "df_max_risk = df_raw[[\"DATE_and_Time\", \"Max_Risk\"]]\n",
        "\n",
        "# ‚úÖ Convert DATE_and_Time to datetime format\n",
        "df_max_risk[\"DATE_and_Time\"] = pd.to_datetime(df_max_risk[\"DATE_and_Time\"])\n",
        "\n",
        "# ‚úÖ Print a sample to verify\n",
        "print(\"‚úÖ Extracted Max_Risk Data Sample:\\n\", df_max_risk.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KNvWoi79ydT",
        "outputId": "dd95809e-3921-4c98-cab5-d5a8c18c3cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Max_Risk data split and saved.\n"
          ]
        }
      ],
      "source": [
        "# Step 4.4.2 ‚úÖ Split into train (2015-2018) and validation (2019)\n",
        "train_max_risk = df_max_risk[df_max_risk[\"DATE_and_Time\"] < \"2019-01-01\"]\n",
        "val_max_risk = df_max_risk[df_max_risk[\"DATE_and_Time\"] >= \"2019-01-01\"]\n",
        "\n",
        "# ‚úÖ Save to CSV for merging\n",
        "train_max_risk.to_csv(\"train_max_risk.csv\", index=False)\n",
        "val_max_risk.to_csv(\"val_max_risk.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Max_Risk data split and saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByiWjas19xW1",
        "outputId": "e76fa870-2e67-4c93-8fe9-58c5c0c49618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Final Row Count - train_XGB: 17819, train_max_risk: 17819\n",
            "‚úÖ Final Row Count - val_XGB: 8760, val_max_risk: 8760\n"
          ]
        }
      ],
      "source": [
        "# Step 4.4.3 ‚úÖ Load XGBoost datasets\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load datasets\n",
        "train_xgb = pd.read_csv(\"train_XGBoost.csv\")\n",
        "val_xgb = pd.read_csv(\"val_XGBoost.csv\")\n",
        "\n",
        "train_max_risk = pd.read_csv(\"train_max_risk.csv\")\n",
        "val_max_risk = pd.read_csv(\"val_max_risk.csv\")\n",
        "\n",
        "# ‚úÖ Remove first 24 rows from train_max_risk to match XGBoost training data\n",
        "train_max_risk = train_max_risk.iloc[24:].reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ Verify row counts again\n",
        "assert len(train_xgb) == len(train_max_risk), \"Fixed row mismatch in train!\"\n",
        "assert len(val_xgb) == len(val_max_risk), \"Validation row count matches!\"\n",
        "\n",
        "print(f\"‚úÖ Final Row Count - train_XGB: {len(train_xgb)}, train_max_risk: {len(train_max_risk)}\")\n",
        "print(f\"‚úÖ Final Row Count - val_XGB: {len(val_xgb)}, val_max_risk: {len(val_max_risk)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7quz-Ry925d",
        "outputId": "f40ad6e4-1076-42c1-fe78-3ce47a057d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Merged Max_Risk with XGBoost train/validate data successfully!\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Step 4.4.4 Merge Max_Risk with XGBoost data\n",
        "train_xgb[\"Max_Risk\"] = train_max_risk[\"Max_Risk\"].values\n",
        "val_xgb[\"Max_Risk\"] = val_max_risk[\"Max_Risk\"].values\n",
        "\n",
        "# ‚úÖ Save updated datasets\n",
        "train_xgb.to_csv(\"train_XGBoost_with_Max_Risk.csv\", index=False)\n",
        "val_xgb.to_csv(\"val_XGBoost_with_Max_Risk.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Merged Max_Risk with XGBoost train/validate data successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot-PuCT591VN"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 4.4.5 Save the new files\n",
        "train_xgb.to_csv(\"train_XGBoost_with_Max_Risk.csv\", index=False)\n",
        "val_xgb.to_csv(\"val_XGBoost_with_Max_Risk.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Recreated and saved train/val datasets.\")\n",
        "\n",
        "# ‚úÖ Immediately download to prevent loss\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"train_XGBoost_with_Max_Risk.csv\")\n",
        "files.download(\"val_XGBoost_with_Max_Risk.csv\")\n",
        "\n",
        "print(\"‚úÖ All files have been downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_xfO_iJ97Kr",
        "outputId": "25495162-4e16-42ad-b99a-46a18b4711d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Validation Data Columns:\n",
            " Index(['Max_Risk_Quartile', 'Max_Risk_Quartile_Lag_2',\n",
            "       'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3',\n",
            "       'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2',\n",
            "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'Max_Risk'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "#Step 4.4.6 Validate data\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Load the trained XGBoost model\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Load the validation dataset\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Max_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Print dataset structure to confirm\n",
        "print(\"‚úÖ Validation Data Columns:\\n\", val_xgb.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DnttLCq-KhD",
        "outputId": "e4e7821c-6ecb-4ab7-969d-1bcebc70b96d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Sample Predicted Max_Risk_Quartile Values:\n",
            " [1.1399602  0.95598    0.74256104 0.94986975 1.4324236 ]\n"
          ]
        }
      ],
      "source": [
        "# Step 4.4.7 ‚úÖ Get the correct feature set used during training\n",
        "features = [col for col in val_xgb.columns if col in xgb_model.feature_names_in_]\n",
        "\n",
        "# ‚úÖ Ensure the feature order matches what XGBoost expects\n",
        "X_val_xgb = val_xgb[features]\n",
        "\n",
        "# ‚úÖ Predict `Max_Risk_Quartile` using XGBoost\n",
        "y_pred_quartile = xgb_model.predict(X_val_xgb)\n",
        "\n",
        "# ‚úÖ Print sample predictions\n",
        "print(\"‚úÖ Sample Predicted Max_Risk_Quartile Values:\\n\", y_pred_quartile[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0lP7Tqs-O6b",
        "outputId": "5a3fd5aa-e478-4db5-ac72-d155342d9492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Added Predicted `Max_Risk_Quartile` to Validation Data\n",
            "   Predicted_Max_Risk_Quartile\n",
            "0                            1\n",
            "1                            1\n",
            "2                            1\n",
            "3                            1\n",
            "4                            1\n"
          ]
        }
      ],
      "source": [
        "#Step 4.4.8 Verify\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load the trained XGBoost model\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Load the validation dataset (without predictions yet)\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Max_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Ensure we use the correct feature set (remove any non-feature columns)\n",
        "features = [col for col in val_xgb.columns if col in xgb_model.feature_names_in_]\n",
        "\n",
        "# ‚úÖ Predict `Max_Risk_Quartile`\n",
        "y_pred_quartile = xgb_model.predict(val_xgb[features])\n",
        "\n",
        "# ‚úÖ Round and clip to ensure values are between 0 and 3\n",
        "y_pred_quartile = y_pred_quartile.round().astype(int)\n",
        "y_pred_quartile = y_pred_quartile.clip(0, 3)\n",
        "\n",
        "# ‚úÖ Add predictions to the validation dataset\n",
        "val_xgb[\"Predicted_Max_Risk_Quartile\"] = y_pred_quartile\n",
        "\n",
        "# ‚úÖ Save the updated validation file\n",
        "val_xgb.to_csv(\"val_XGBoost_with_Predicted_Quartile.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Print confirmation and sample rows\n",
        "print(\"‚úÖ Added Predicted `Max_Risk_Quartile` to Validation Data\")\n",
        "print(val_xgb[[\"Predicted_Max_Risk_Quartile\"]].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB7QiuPBNNuw",
        "outputId": "c59c17e7-13d1-45e3-d497-9d0970fa755b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Validation Data Columns:\n",
            " Index(['Max_Risk_Quartile', 'Max_Risk_Quartile_Lag_2',\n",
            "       'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3',\n",
            "       'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2',\n",
            "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'Max_Risk'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Load the validation dataset\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Max_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Print dataset structure to confirm\n",
        "print(\"‚úÖ Validation Data Columns:\\n\", val_xgb.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lhE1prD-aKb",
        "outputId": "6a950f6b-5829-4b7c-b8a8-418776c779d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ KNN model trained to learn the relationship between `Max_Risk_Quartile` and `Max_Risk`.\n",
            "‚úÖ Converted `Max_Risk_Quartile` into `Predicted_Max_Risk` using KNN.\n",
            "   Max_Risk_Quartile  Predicted_Max_Risk\n",
            "0                  0            2.216936\n",
            "1                  1            6.988404\n",
            "2                  2          116.324243\n",
            "3                  1            6.988404\n",
            "4                  2          116.324243\n"
          ]
        }
      ],
      "source": [
        "# Step 4.4.9 Learn the relationships between quartile and max risk raw\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ‚úÖ Load training data with Max_Risk\n",
        "train_xgb = pd.read_csv(\"train_XGBoost_with_Max_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Define the features and target for KNN\n",
        "features_for_knn = [\"Max_Risk_Quartile\"]\n",
        "target_for_knn = \"Max_Risk\"\n",
        "\n",
        "# ‚úÖ Standardize the features before applying KNN\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train_xgb[features_for_knn])  # Fit on training data\n",
        "\n",
        "# ‚úÖ Train KNN to predict `Max_Risk` from `Max_Risk_Quartile`\n",
        "knn = KNeighborsRegressor(n_neighbors=3)  # Using 3 nearest neighbors\n",
        "knn.fit(train_scaled, train_xgb[target_for_knn])\n",
        "\n",
        "print(\"‚úÖ KNN model trained to learn the relationship between `Max_Risk_Quartile` and `Max_Risk`.\")\n",
        "\n",
        "# ‚úÖ Load validation data\n",
        "val_xgb = pd.read_csv(\"val_XGBoost.csv\")\n",
        "\n",
        "# ‚úÖ Ensure column names are consistent by renaming\n",
        "# Only rename if necessary, and check the current column names\n",
        "if \"Predicted_Max_Risk_Quartile\" in val_xgb.columns:\n",
        "    val_xgb.rename(columns={\"Predicted_Max_Risk_Quartile\": \"Max_Risk_Quartile\"}, inplace=True)\n",
        "\n",
        "# ‚úÖ Standardize the validation data using the same scaler as the training data\n",
        "val_scaled = scaler.transform(val_xgb[features_for_knn])  # Use the same scaler for validation\n",
        "\n",
        "# ‚úÖ Use KNN to estimate `Max_Risk` for the validation data\n",
        "val_xgb[\"Predicted_Max_Risk\"] = knn.predict(val_scaled)\n",
        "\n",
        "# ‚úÖ Save the updated validation file\n",
        "val_xgb.to_csv(\"val_XGBoost_with_Predicted_Max_Risk.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Print confirmation and sample rows\n",
        "print(\"‚úÖ Converted `Max_Risk_Quartile` into `Predicted_Max_Risk` using KNN.\")\n",
        "print(val_xgb[[\"Max_Risk_Quartile\", \"Predicted_Max_Risk\"]].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bT8Eo86NXLG",
        "outputId": "9c52abd2-7207-40f5-f5a3-73fae2063638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Validation Data Columns:\n",
            " Index(['Max_Risk_Quartile', 'Max_Risk_Quartile_Lag_2',\n",
            "       'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3',\n",
            "       'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2',\n",
            "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'Max_Risk'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Step 4.4.9.1 ‚úÖ Load the validation dataset\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Max_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Print dataset structure to confirm\n",
        "print(\"‚úÖ Validation Data Columns:\\n\", val_xgb.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y-BOGzH-kPD",
        "outputId": "54ad5351-7fd2-4883-e04c-0c714e0652e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Validation Data Columns:\n",
            " Index(['Max_Risk_Quartile', 'Max_Risk_Quartile_Lag_2',\n",
            "       'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3',\n",
            "       'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2',\n",
            "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'Predicted_Max_Risk'],\n",
            "      dtype='object')\n",
            "   Max_Risk_Quartile  Max_Risk_Quartile_Lag_2  Max_Risk_Quartile_Lag_24  \\\n",
            "0                  0                      2.0                       2.0   \n",
            "1                  1                      1.0                       0.0   \n",
            "2                  2                      0.0                       2.0   \n",
            "3                  1                      1.0                       2.0   \n",
            "4                  2                      2.0                       3.0   \n",
            "\n",
            "   Wet_Bulb_Lag_2  Wet_Bulb_Lag_3  Wind_Speed_Lag_2  Altimeter_Lag_3  \\\n",
            "0        0.352941        0.372549          0.270270         0.765823   \n",
            "1        0.352941        0.352941          0.216216         0.778481   \n",
            "2        0.254902        0.352941          0.162162         0.791139   \n",
            "3        0.274510        0.254902          0.000000         0.848101   \n",
            "4        0.294118        0.274510          0.135135         0.860759   \n",
            "\n",
            "   Wind_Direction_Lag_2  hour_sin  hour_cos  month_sin  month_cos  \\\n",
            "0              0.986111  0.000000  1.000000        0.5   0.866025   \n",
            "1              0.083333  0.258819  0.965926        0.5   0.866025   \n",
            "2              0.333333  0.500000  0.866025        0.5   0.866025   \n",
            "3              0.000000  0.707107  0.707107        0.5   0.866025   \n",
            "4              0.333333  0.866025  0.500000        0.5   0.866025   \n",
            "\n",
            "   Predicted_Max_Risk  \n",
            "0            2.216936  \n",
            "1            6.988404  \n",
            "2          116.324243  \n",
            "3            6.988404  \n",
            "4          116.324243  \n"
          ]
        }
      ],
      "source": [
        "# Step 4.4.9.2 Verify transformation\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Reload the validation dataset with Predicted_Max_Risk\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Predicted_Max_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Print sample to confirm correct data\n",
        "print(\"‚úÖ Validation Data Columns:\\n\", val_xgb.columns)\n",
        "print(val_xgb.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.4.9.2.1 add max risk to final file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def categorize_risk(value):\n",
        "    if value < 301:\n",
        "        return 0  # Low Risk\n",
        "    elif value < 4001:\n",
        "        return 1  # Moderate Risk\n",
        "    else:\n",
        "        return 2  # Severe Risk\n",
        "\n",
        "# ‚úÖ Load the original validation dataset with Max_Risk (the original validation file)\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Max_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Check that Max_Risk is in the validation data\n",
        "print(val_xgb.columns)\n",
        "\n",
        "# ‚úÖ If 'Predicted_Max_Risk' is in the dataframe, we use the trained model to predict it\n",
        "val_scaled = scaler.transform(val_xgb[[\"Max_Risk_Quartile\"]])  # Use the same scaler as before\n",
        "val_xgb[\"Predicted_Max_Risk\"] = knn.predict(val_scaled)  # Predict Max_Risk for the validation data\n",
        "\n",
        "# ‚úÖ Save the updated validation file, ensuring Max_Risk and Predicted_Max_Risk are present\n",
        "val_xgb.to_csv(\"val_XGBoost_with_Predicted_Max_Risk.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Apply categorization to Max_Risk and Predicted_Max_Risk\n",
        "val_xgb[\"Actual_Risk\"] = val_xgb[\"Max_Risk\"].apply(categorize_risk)\n",
        "val_xgb[\"Predicted_Risk\"] = val_xgb[\"Predicted_Max_Risk\"].apply(categorize_risk)\n",
        "\n",
        "# ‚úÖ Save the updated validation file with categorical risk labels\n",
        "val_xgb.to_csv(\"val_XGBoost_with_Categorical_Risk.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Print confirmation and sample rows\n",
        "print(\"‚úÖ Converted `Max_Risk` and `Predicted_Max_Risk` into categorical risk levels.\")\n",
        "print(val_xgb[[\"Max_Risk\", \"Predicted_Max_Risk\", \"Actual_Risk\", \"Predicted_Risk\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzG8atmDF23L",
        "outputId": "0fc32037-c5ba-4999-c1ee-9af3eaf95b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Max_Risk_Quartile', 'Max_Risk_Quartile_Lag_2',\n",
            "       'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3',\n",
            "       'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2',\n",
            "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'Max_Risk'],\n",
            "      dtype='object')\n",
            "‚úÖ Converted `Max_Risk` and `Predicted_Max_Risk` into categorical risk levels.\n",
            "    Max_Risk  Predicted_Max_Risk  Actual_Risk  Predicted_Risk\n",
            "0   2.306530            2.216936            0               0\n",
            "1   5.107092            6.988404            0               0\n",
            "2  89.793750          116.324243            0               0\n",
            "3  18.688470            6.988404            0               0\n",
            "4  25.992310          116.324243            0               0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dahyLH42-tdk",
        "outputId": "9d114f18-315e-4825-e2b7-ae2b4a7df55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 'Max_Risk' column exists in the validation data.\n",
            "‚úÖ Converted `Max_Risk` and `Predicted_Max_Risk` into categorical risk levels.\n",
            "    Max_Risk  Predicted_Max_Risk  Actual_Risk  Predicted_Risk\n",
            "0   2.306530            2.216936            0               0\n",
            "1   5.107092            6.988404            0               0\n",
            "2  89.793750          116.324243            0               0\n",
            "3  18.688470            6.988404            0               0\n",
            "4  25.992310          116.324243            0               0\n"
          ]
        }
      ],
      "source": [
        "# 4.4.9.3 ‚úÖ Define risk classification function\n",
        "\n",
        "\n",
        "# ‚úÖ Check if 'Max_Risk' column exists\n",
        "if 'Max_Risk' not in val_xgb.columns:\n",
        "    print(\"‚ùå 'Max_Risk' column is missing in the validation data.\")\n",
        "else:\n",
        "    print(\"‚úÖ 'Max_Risk' column exists in the validation data.\")\n",
        "\n",
        "    # ‚úÖ Apply categorization\n",
        "    val_xgb[\"Actual_Risk\"] = val_xgb[\"Max_Risk\"].apply(categorize_risk)\n",
        "    val_xgb[\"Predicted_Risk\"] = val_xgb[\"Predicted_Max_Risk\"].apply(categorize_risk)\n",
        "\n",
        "    # ‚úÖ Save and confirm\n",
        "    val_xgb.to_csv(\"val_XGBoost_with_Categorical_Risk.csv\", index=False)\n",
        "    print(\"‚úÖ Converted `Max_Risk` and `Predicted_Max_Risk` into categorical risk levels.\")\n",
        "    print(val_xgb[[\"Max_Risk\", \"Predicted_Max_Risk\", \"Actual_Risk\", \"Predicted_Risk\"]].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8hrb-sob-xaz",
        "outputId": "c813c1be-0e09-4737-affd-f92ba05fd568"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_50e29de7-c05d-401a-8947-7731326e3329\", \"val_XGBoost_with_Categorical_Risk.csv\", 1845758)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download the new file\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"val_XGBoost_with_Categorical_Risk.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxiBUTtI-05D",
        "outputId": "e9b8313a-4a24-4719-c9e7-d6cb327622b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Validation Data Sample:\n",
            "     Max_Risk  Predicted_Max_Risk  Actual_Risk  Predicted_Risk\n",
            "0   2.306530            2.216936            0               0\n",
            "1   5.107092            6.988404            0               0\n",
            "2  89.793750          116.324243            0               0\n",
            "3  18.688470            6.988404            0               0\n",
            "4  25.992310          116.324243            0               0\n"
          ]
        }
      ],
      "source": [
        "# Step 4.4.9.4 Verify structure\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load the validation dataset that contains categorical risk levels\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Print sample to confirm structure\n",
        "print(\"‚úÖ Validation Data Sample:\\n\", val_xgb[[\"Max_Risk\", \"Predicted_Max_Risk\", \"Actual_Risk\", \"Predicted_Risk\"]].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFvpKSI6y_6p"
      },
      "source": [
        "Ensemble testing. For the XGBoost, a separate file was created to add max_Risk into the training set. A KNN was then trained to learn the quartile definitions. The KNN then predicted Max_Risk based on the XGBoost Quartile Risk predictions. These were converted into categorical risk and tested against known risk. This proved to be a highly accurate way to convert the Regression model to categorical for ensembling. The suspected reason for the accuracy is that the KNN only had to be in the ranges of Low, Moderate, and Severe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYxZQb9Z045f"
      },
      "outputs": [],
      "source": [
        "#Step 5.1.1: Ensemble Threshold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# ‚úÖ Load trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Load validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "X_val_lstm = np.array(X_val_lstm, dtype=np.float32)\n",
        "\n",
        "# ‚úÖ Load XGBoost validation data with improved categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Get LSTM Predictions\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)  # Probabilities for Low, Moderate, Severe\n",
        "lstm_preds = np.argmax(lstm_probs, axis=1)  # Convert to class labels\n",
        "\n",
        "# ‚úÖ Use the already validated XGBoost categorical risk predictions\n",
        "xgb_preds = val_xgb[\"Predicted_Risk\"].values\n",
        "\n",
        "# ‚úÖ Apply Final Optimized Rule-Based Fusion for Ensemble Predictions\n",
        "final_preds = []\n",
        "for i in range(len(lstm_preds)):\n",
        "    lstm_pred = lstm_preds[i]\n",
        "    xgb_pred = xgb_preds[i]\n",
        "\n",
        "    if lstm_pred == 0:\n",
        "        final_preds.append(0)  # ‚úÖ Always trust LSTM for Low (recall = 82%)\n",
        "    elif xgb_pred == 2 and lstm_pred != 0:\n",
        "        final_preds.append(2)  # ‚úÖ Trust XGBoost for Severe unless LSTM says Low\n",
        "    elif xgb_pred == 1 and lstm_pred == 1:\n",
        "        final_preds.append(1)  # ‚úÖ If both predict Moderate, trust Moderate\n",
        "    elif lstm_pred == 1 and xgb_pred == 2:\n",
        "        final_preds.append(1)  # ‚úÖ If LSTM says Moderate and XGBoost says Severe, trust Moderate\n",
        "    elif lstm_pred == 2 and xgb_pred == 1:\n",
        "        final_preds.append(1)  # ‚úÖ If LSTM says Severe but XGBoost says Moderate, trust Moderate\n",
        "    elif lstm_pred == 2 and xgb_pred == 0:\n",
        "        final_preds.append(1)  # ‚úÖ If LSTM says Severe but XGBoost says Low, default to Moderate\n",
        "    elif lstm_pred == 1 and xgb_pred == 0:\n",
        "        final_preds.append(0)  # ‚úÖ If LSTM says Moderate but XGBoost says Low, trust Low\n",
        "    else:\n",
        "        final_preds.append(xgb_pred)  # ‚úÖ Default to XGBoost when uncertain\n",
        "\n",
        "# ‚úÖ Convert final predictions to NumPy array\n",
        "final_preds = np.array(final_preds)\n",
        "\n",
        "\n",
        "# ‚úÖ Convert one-hot encoded y_val_lstm back to class labels\n",
        "y_val_lstm_labels = np.argmax(y_val_lstm, axis=1)\n",
        "\n",
        "\n",
        "# ‚úÖ Evaluate Ensemble Performance\n",
        "# ‚úÖ Evaluate Ensemble Performance\n",
        "ensemble_accuracy = accuracy_score(y_val_lstm_labels, final_preds)\n",
        "ensemble_cm = confusion_matrix(y_val_lstm_labels, final_preds)\n",
        "ensemble_report = classification_report(y_val_lstm_labels, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "\n",
        "# ‚úÖ Convert confusion matrix to DataFrame for visualization\n",
        "ensemble_cm_df = pd.DataFrame(ensemble_cm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                              columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Plot confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Further Updated Ensemble Risk Prediction Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Print classification report\n",
        "print(f\"‚úÖ Further Updated Ensemble Accuracy: {ensemble_accuracy:.4%}\")\n",
        "print(\"‚úÖ Further Updated Confusion Matrix:\\n\", ensemble_cm)\n",
        "print(\"‚úÖ Further Updated Classification Report:\\n\", ensemble_report)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8St_DxP4w0Rt"
      },
      "outputs": [],
      "source": [
        "#5.1.1.1 Confidence thesholding\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# ‚úÖ Load trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Load validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "X_val_lstm = np.array(X_val_lstm, dtype=np.float32)\n",
        "\n",
        "# ‚úÖ Load XGBoost validation data with improved categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Get LSTM Predictions\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)  # Probabilities for Low, Moderate, Severe\n",
        "lstm_preds = np.argmax(lstm_probs, axis=1)  # Convert to class labels\n",
        "\n",
        "# ‚úÖ Use the already validated XGBoost categorical risk predictions\n",
        "xgb_preds = val_xgb[\"Predicted_Risk\"].values\n",
        "\n",
        "# ‚úÖ Apply confidence-based decision rule\n",
        "threshold = 0.75  # Confidence threshold\n",
        "\n",
        "final_preds = []\n",
        "for i in range(len(lstm_preds)):\n",
        "    lstm_pred = np.argmax(lstm_probs[i])\n",
        "    xgb_pred = np.argmax(xgb_probs_trimmed[i])\n",
        "\n",
        "    xgb_confidence = np.max(xgb_probs_trimmed[i])  # Get XGBoost‚Äôs highest probability\n",
        "\n",
        "    if xgb_confidence >= threshold:\n",
        "        final_preds.append(xgb_pred)  # ‚úÖ Trust XGBoost if highly confident\n",
        "    else:\n",
        "        final_preds.append(lstm_pred)  # ‚úÖ Default to LSTM if XGBoost isn‚Äôt confident\n",
        "\n",
        "final_preds = np.array(final_preds)\n",
        "\n",
        "\n",
        "\n",
        "# ‚úÖ Convert one-hot encoded y_val_lstm back to class labels\n",
        "y_val_lstm_labels = np.argmax(y_val_lstm, axis=1)\n",
        "\n",
        "\n",
        "# ‚úÖ Evaluate Ensemble Performance\n",
        "ensemble_accuracy = accuracy_score(y_val_lstm_labels, final_preds)\n",
        "ensemble_cm = confusion_matrix(y_val_lstm_labels, final_preds)\n",
        "ensemble_report = classification_report(y_val_lstm_labels, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "\n",
        "# ‚úÖ Convert confusion matrix to DataFrame for visualization\n",
        "ensemble_cm_df = pd.DataFrame(ensemble_cm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                              columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Plot confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Further Updated Ensemble Risk Prediction Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Print classification report\n",
        "print(f\"‚úÖ Further Updated Ensemble Accuracy: {ensemble_accuracy:.4%}\")\n",
        "print(\"‚úÖ Further Updated Confusion Matrix:\\n\", ensemble_cm)\n",
        "print(\"‚úÖ Further Updated Classification Report:\\n\", ensemble_report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJnVwaglOplZ"
      },
      "outputs": [],
      "source": [
        "#5.1.2 Averaging using probabilities (when LSTM leads, the model gets worse. XGBoost should always be at least 50% of the estimation)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# ‚úÖ Load trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Load validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "X_val_lstm = np.array(X_val_lstm, dtype=np.float32)\n",
        "\n",
        "# ‚úÖ Load XGBoost validation data with improved categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Get LSTM Predictions\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)  # Probabilities for Low, Moderate, Severe\n",
        "lstm_preds = np.argmax(lstm_probs, axis=1)  # Convert to class labels\n",
        "\n",
        "# ‚úÖ Use the already validated XGBoost categorical risk predictions\n",
        "xgb_preds = val_xgb[\"Predicted_Risk\"].values\n",
        "\n",
        "#Step 1 ‚úÖ Extract probabilities from LSTM model\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)  # Probabilities for Low, Moderate, Severe\n",
        "\n",
        "# ‚úÖ Convert XGBoost categorical predictions to probability-like values\n",
        "xgb_probs = np.zeros((len(xgb_preds), 3))  # Initialize probability array\n",
        "for i, pred in enumerate(xgb_preds):\n",
        "    xgb_probs[i, pred] = 1  # Assign full probability to the predicted category\n",
        "\n",
        "# ‚úÖ Trim XGBoost predictions to match LSTM sample count\n",
        "xgb_probs_trimmed = xgb_probs[-8664:]\n",
        "\n",
        "#Step 2 apply weights\n",
        "\n",
        "# ‚úÖ Compute ensemble probabilities using weighted averaging\n",
        "# ‚úÖ Compute ensemble probabilities using weighted averaging\n",
        "ensemble_probs = (0.6 * lstm_probs) + (0.4 * xgb_probs_trimmed)  # Ensure trimmed version is used\n",
        "  # Adjust weights if needed\n",
        "\n",
        "# ‚úÖ Convert probabilities into final predictions\n",
        "final_preds = np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "#Step 3 Evaluate Averaging Based Ensemble\n",
        "# ‚úÖ Evaluate Ensemble Performance\n",
        "ensemble_accuracy = accuracy_score(y_val_lstm_labels, final_preds)\n",
        "ensemble_cm = confusion_matrix(y_val_lstm_labels, final_preds)\n",
        "ensemble_report = classification_report(y_val_lstm_labels, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "\n",
        "# ‚úÖ Convert confusion matrix to DataFrame for visualization\n",
        "ensemble_cm_df = pd.DataFrame(ensemble_cm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                              columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Plot confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Averaging-Based Ensemble Risk Prediction Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Plot confusion matrix (Percentages)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_percent_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Averaging-Based Ensemble Risk Prediction Confusion Matrix (Percentages)\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Print classification report\n",
        "print(f\"‚úÖ Averaging-Based Ensemble Accuracy: {ensemble_accuracy:.4%}\")\n",
        "print(\"‚úÖ Averaging-Based Confusion Matrix:\\n\", ensemble_cm)\n",
        "print(\"‚úÖ Averaging-Based Classification Report:\\n\", ensemble_report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPtSfNKnzuBI"
      },
      "source": [
        "Confidence threshold and averaging each category ended up producing the same results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh1tgI8vcF-w"
      },
      "outputs": [],
      "source": [
        "#5.1.3 Averaging with different weights (Selected as Champion for Southern Airfields only based on accuracy, sensitivity, and specificity), otherwise Logarithmic was better\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ‚úÖ Reload trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Reload validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "X_val_lstm = np.array(X_val_lstm, dtype=np.float32)\n",
        "\n",
        "# ‚úÖ Reload XGBoost validation data with categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Reload final ensemble probabilities (if previously saved)\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)\n",
        "xgb_probs = np.zeros((len(val_xgb), 3))  # Initialize probability array\n",
        "for i, pred in enumerate(val_xgb[\"Predicted_Risk\"]):\n",
        "    xgb_probs[i, pred] = 1  # Assign full probability to the predicted category\n",
        "\n",
        "# ‚úÖ Trim XGBoost predictions to match LSTM sample count\n",
        "xgb_probs_trimmed = xgb_probs[-8664:]\n",
        "print(\"‚úÖ Reloaded all necessary data for ensemble optimization.\")\n",
        "\n",
        "# ‚úÖ Adjust Weighting for Higher Accuracy (Shift More Weight to LSTM)\n",
        "ensemble_probs = (0.4 * lstm_probs) + (0.6 * xgb_probs_trimmed)  # Adjust weights\n",
        "\n",
        "# ‚úÖ Convert probabilities into final predictions\n",
        "final_preds = np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "# ‚úÖ Convert one-hot encoded y_val_lstm back to class labels\n",
        "y_val_lstm_labels = np.argmax(y_val_lstm, axis=1)  # Convert one-hot encoding to class labels\n",
        "\n",
        "# ‚úÖ Evaluate Ensemble Performance\n",
        "ensemble_accuracy = accuracy_score(y_val_lstm_labels, final_preds)\n",
        "ensemble_cm = confusion_matrix(y_val_lstm_labels, final_preds)\n",
        "ensemble_report = classification_report(y_val_lstm_labels, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "\n",
        "# ‚úÖ Convert confusion matrix to DataFrame for visualization\n",
        "ensemble_cm_df = pd.DataFrame(ensemble_cm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                              columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Compute the confusion matrix as percentages\n",
        "ensemble_cm_percentage = ensemble_cm.astype(np.float32) / ensemble_cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "# ‚úÖ Convert to DataFrame for visualization\n",
        "ensemble_cm_percent_df = pd.DataFrame(ensemble_cm_percentage, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                                      columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Plot confusion matrix (Absolute Counts)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Final Meta-Ensemble Confusion Matrix (Counts)\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Plot confusion matrix (Percentages)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_percent_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Final Meta-Ensemble Confusion Matrix (Percentages)\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Print classification report and accuracy\n",
        "print(f\"‚úÖ Final Meta-Ensemble Accuracy: {ensemble_accuracy:.4%}\")\n",
        "print(\"‚úÖ Final Meta-Ensemble Confusion Matrix:\\n\", ensemble_cm)\n",
        "print(\"‚úÖ Final Meta-Ensemble Classification Report:\\n\", ensemble_report)\n",
        "\n",
        "# ‚úÖ Define the meta-model (ensemble) - meta_model is now a class object\n",
        "class MetaEnsembleModel:\n",
        "    def __init__(self, lstm_model, xgb_model, ensemble_weights=(0.4, 0.6)):\n",
        "        self.lstm_model = lstm_model\n",
        "        self.xgb_model = xgb_model\n",
        "        self.ensemble_weights = ensemble_weights\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Generate LSTM probabilities\n",
        "        lstm_probs = self.lstm_model.predict(X)\n",
        "\n",
        "        # Generate XGBoost predictions and convert them to probabilities (one-hot)\n",
        "        xgb_probs = np.zeros((len(X), 3))  # Assuming 3 classes (Low, Moderate, Severe)\n",
        "        for i, pred in enumerate(self.xgb_model.predict(X)):\n",
        "            xgb_probs[i, pred] = 1  # Assign full probability to the predicted class\n",
        "\n",
        "        # Combine predictions using weighted average\n",
        "        ensemble_probs = (self.ensemble_weights[0] * lstm_probs) + (self.ensemble_weights[1] * xgb_probs)\n",
        "\n",
        "        # Return the final predictions\n",
        "        return np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "# ‚úÖ Save the final Adjusted Averaging-Based Ensemble Model\n",
        "meta_model = MetaEnsembleModel(lstm_model, xgb_model, ensemble_weights=(0.4, 0.6))\n",
        "joblib.dump(meta_model, \"final_best_ensemble_model.pkl\")\n",
        "\n",
        "# ‚úÖ Download the saved model to your computer\n",
        "from google.colab import files\n",
        "files.download(\"final_best_ensemble_model.pkl\")\n",
        "\n",
        "print(\"‚úÖ Final Best Ensemble Model saved successfully.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I27jo37AeKhl"
      },
      "outputs": [],
      "source": [
        "#5.1.3.1 Averaging with different weights (more on XGBoost)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ‚úÖ Reload trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Reload validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# Ensure that X_val_lstm is in float32 format\n",
        "X_val_lstm = X_val_lstm.astype(np.float32)\n",
        "\n",
        "# ‚úÖ Reload XGBoost validation data with categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Reload final ensemble probabilities (if previously saved)\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)\n",
        "xgb_probs = np.zeros((len(val_xgb), 3))  # Initialize probability array\n",
        "for i, pred in enumerate(val_xgb[\"Predicted_Risk\"]):\n",
        "    xgb_probs[i, pred] = 1  # Assign full probability to the predicted category\n",
        "# ‚úÖ Trim XGBoost predictions to match LSTM sample count\n",
        "xgb_probs_trimmed = xgb_probs[-8664:]\n",
        "print(\"‚úÖ Reloaded all necessary data for ensemble optimization.\")\n",
        "# ‚úÖ Adjust Weighting for Higher Accuracy (Shift More Weight to LSTM)\n",
        "ensemble_probs = (0.55 * lstm_probs) + (0.45 * xgb_probs_trimmed)  # Adjust weights\n",
        "\n",
        "# ‚úÖ Convert probabilities into final predictions\n",
        "final_preds = np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "# ‚úÖ Evaluate Ensemble Performance\n",
        "ensemble_accuracy = accuracy_score(y_val_lstm_labels, final_preds)\n",
        "ensemble_cm = confusion_matrix(y_val_lstm_labels, final_preds)\n",
        "ensemble_report = classification_report(y_val_lstm_labels, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "\n",
        "# ‚úÖ Convert confusion matrix to DataFrame for visualization\n",
        "ensemble_cm_df = pd.DataFrame(ensemble_cm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                              columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Compute the confusion matrix as percentages\n",
        "ensemble_cm_percentage = ensemble_cm.astype(np.float32) / ensemble_cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "# ‚úÖ Convert to DataFrame for visualization\n",
        "ensemble_cm_percent_df = pd.DataFrame(ensemble_cm_percentage, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                                      columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Plot confusion matrix (Absolute Counts)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Averaging-Based Ensemble Risk Prediction Confusion Matrix (Counts)\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Plot confusion matrix (Percentages)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_percent_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Averaging-Based Ensemble Risk Prediction Confusion Matrix (Percentages)\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Print classification report\n",
        "print(f\"‚úÖ Adjusted Averaging-Based Ensemble Accuracy: {ensemble_accuracy:.4%}\")\n",
        "print(\"‚úÖ Adjusted Averaging-Based Confusion Matrix:\\n\", ensemble_cm)\n",
        "print(\"‚úÖ Adjusted Averaging-Based Classification Report:\\n\", ensemble_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ATyXOA7iklJ"
      },
      "outputs": [],
      "source": [
        "#5.1.4 Averaging weights on specific categories\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ‚úÖ Reload trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Reload validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "# Ensure that X_val_lstm is in float32 format\n",
        "X_val_lstm = X_val_lstm.astype(np.float32)\n",
        "\n",
        "# ‚úÖ Reload XGBoost validation data with categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Reload final ensemble probabilities (if previously saved)\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)\n",
        "xgb_probs = np.zeros((len(val_xgb), 3))  # Initialize probability array\n",
        "for i, pred in enumerate(val_xgb[\"Predicted_Risk\"]):\n",
        "    xgb_probs[i, pred] = 1  # Assign full probability to the predicted category\n",
        "# ‚úÖ Trim XGBoost predictions to match LSTM sample count\n",
        "xgb_probs_trimmed = xgb_probs[-8664:]\n",
        "print(\"‚úÖ Reloaded all necessary data for ensemble optimization.\")\n",
        "\n",
        "\n",
        "# ‚úÖ Create category-specific weights\n",
        "low_weight_lstm, low_weight_xgb = 0.7, 0.3  # Prioritize LSTM for Low Risk\n",
        "moderate_weight_lstm, moderate_weight_xgb = 0.4, 0.6  # Balance for Moderate Risk\n",
        "severe_weight_lstm, severe_weight_xgb = 0.3, 0.7  # Prioritize XGBoost for Severe Risk\n",
        "\n",
        "# ‚úÖ Apply category-specific weights\n",
        "ensemble_probs = np.zeros_like(lstm_probs)\n",
        "ensemble_probs[:, 0] = (low_weight_lstm * lstm_probs[:, 0]) + (low_weight_xgb * xgb_probs_trimmed[:, 0])  # Low Risk\n",
        "ensemble_probs[:, 1] = (moderate_weight_lstm * lstm_probs[:, 1]) + (moderate_weight_xgb * xgb_probs_trimmed[:, 1])  # Moderate Risk\n",
        "ensemble_probs[:, 2] = (severe_weight_lstm * lstm_probs[:, 2]) + (severe_weight_xgb * xgb_probs_trimmed[:, 2])  # Severe Risk\n",
        "\n",
        "# ‚úÖ Convert probabilities into final predictions\n",
        "final_preds = np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "# ‚úÖ Evaluate Ensemble Performance\n",
        "ensemble_accuracy = accuracy_score(y_val_lstm_labels, final_preds)\n",
        "ensemble_cm = confusion_matrix(y_val_lstm_labels, final_preds)\n",
        "ensemble_report = classification_report(y_val_lstm_labels, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "\n",
        "# ‚úÖ Convert confusion matrix to DataFrame for visualization\n",
        "ensemble_cm_df = pd.DataFrame(ensemble_cm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                              columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Compute the confusion matrix as percentages\n",
        "ensemble_cm_percentage = ensemble_cm.astype(np.float32) / ensemble_cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "# ‚úÖ Convert to DataFrame for visualization\n",
        "ensemble_cm_percent_df = pd.DataFrame(ensemble_cm_percentage, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                                      columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Plot confusion matrix (Absolute Counts)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Custom-Weighted Ensemble Risk Prediction Confusion Matrix (Counts)\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Plot confusion matrix (Percentages)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_percent_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Custom-Weighted Ensemble Risk Prediction Confusion Matrix (Percentages)\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Print classification report\n",
        "print(f\"‚úÖ Custom-Weighted Ensemble Accuracy: {ensemble_accuracy:.4%}\")\n",
        "print(\"‚úÖ Custom-Weighted Ensemble Confusion Matrix:\\n\", ensemble_cm)\n",
        "print(\"‚úÖ Custom-Weighted Ensemble Classification Report:\\n\", ensemble_report)\n",
        "\n",
        "import joblib\n",
        "\n",
        "# ‚úÖ Save the final Custom-Weighted Ensemble Model\n",
        "joblib.dump(meta_model, \"best_custom_weighted_ensemble.pkl\")\n",
        "\n",
        "# ‚úÖ Download the saved model to your computer\n",
        "from google.colab import files\n",
        "files.download(\"best_custom_weighted_ensemble.pkl\")\n",
        "\n",
        "print(\"‚úÖ Best Custom-Weighted Ensemble Model (81.90%) saved and ready for download.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCNmJqu3wOaf",
        "outputId": "a2ebc5ed-42f5-4e7b-ce97-b83fde1ec7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m271/271\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 56ms/step\n",
            "‚úÖ Reloaded all necessary data for ensemble optimization.\n",
            "‚úÖ Best Weights: Low = 0.7, Moderate = 0.4, Severe = 0.3\n",
            "‚úÖ Best Accuracy: 92.1168%\n"
          ]
        }
      ],
      "source": [
        "#5.1.5 Averaging weights on specific categories with automatic weights\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ‚úÖ Reload trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Reload validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# Ensure that X_val_lstm is in float32 format\n",
        "X_val_lstm = X_val_lstm.astype(np.float32)\n",
        "\n",
        "# ‚úÖ Reload XGBoost validation data with categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Reload final ensemble probabilities (if previously saved)\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)\n",
        "xgb_probs = np.zeros((len(val_xgb), 3))  # Initialize probability array\n",
        "for i, pred in enumerate(val_xgb[\"Predicted_Risk\"]):\n",
        "    xgb_probs[i, pred] = 1  # Assign full probability to the predicted category\n",
        "# ‚úÖ Trim XGBoost predictions to match LSTM sample count\n",
        "xgb_probs_trimmed = xgb_probs[-8664:]\n",
        "print(\"‚úÖ Reloaded all necessary data for ensemble optimization.\")\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ‚úÖ Define weight search space\n",
        "low_weights = [0.7, 0.75, 0.8]  # Weights for LSTM in Low Risk\n",
        "moderate_weights = [0.4, 0.5, 0.6]  # Weights for LSTM in Moderate Risk\n",
        "severe_weights = [0.3, 0.4, 0.5]  # Weights for LSTM in Severe Risk\n",
        "\n",
        "best_accuracy = 0\n",
        "best_weights = None\n",
        "\n",
        "# ‚úÖ Try all combinations of weights\n",
        "for lw, mw, sw in itertools.product(low_weights, moderate_weights, severe_weights):\n",
        "    ensemble_probs = np.zeros_like(lstm_probs)\n",
        "    ensemble_probs[:, 0] = (lw * lstm_probs[:, 0]) + ((1 - lw) * xgb_probs_trimmed[:, 0])  # Low\n",
        "    ensemble_probs[:, 1] = (mw * lstm_probs[:, 1]) + ((1 - mw) * xgb_probs_trimmed[:, 1])  # Moderate\n",
        "    ensemble_probs[:, 2] = (sw * lstm_probs[:, 2]) + ((1 - sw) * xgb_probs_trimmed[:, 2])  # Severe\n",
        "\n",
        "    final_preds = np.argmax(ensemble_probs, axis=1)\n",
        "    accuracy = accuracy_score(y_val_lstm_labels, final_preds)\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_weights = (lw, mw, sw)\n",
        "\n",
        "print(f\"‚úÖ Best Weights: Low = {best_weights[0]}, Moderate = {best_weights[1]}, Severe = {best_weights[2]}\")\n",
        "print(f\"‚úÖ Best Accuracy: {best_accuracy:.4%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN-VOAh80DIf"
      },
      "source": [
        "Logistic regression meta ensemble predicted it would be the most accurate, however, it was less accurate than Weighted averages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ_TH_dpwner"
      },
      "outputs": [],
      "source": [
        "#5.1.6 Logistic Regression Meta Ensemble, overall champion model because it worked well with all airfields.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# ‚úÖ Reload trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Reload validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# Ensure that X_val_lstm is in float32 format\n",
        "X_val_lstm = X_val_lstm.astype(np.float32)\n",
        "\n",
        "# ‚úÖ Reload XGBoost validation data with categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Reload final ensemble probabilities (if previously saved)\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)\n",
        "xgb_probs = np.zeros((len(val_xgb), 3))  # Initialize probability array\n",
        "for i, pred in enumerate(val_xgb[\"Predicted_Risk\"]):\n",
        "    xgb_probs[i, pred] = 1  # Assign full probability to the predicted category\n",
        "\n",
        "# ‚úÖ Trim XGBoost predictions to match LSTM sample count\n",
        "xgb_probs_trimmed = xgb_probs[-8664:]\n",
        "print(\"‚úÖ Reloaded all necessary data for ensemble optimization.\")\n",
        "\n",
        "# ‚úÖ Convert one-hot encoded y_val_lstm back to class labels\n",
        "y_val_lstm_labels = np.argmax(y_val_lstm, axis=1)  # Convert one-hot encoding to class labels\n",
        "\n",
        "# ‚úÖ Create dataset using LSTM and XGBoost probabilities as features\n",
        "meta_X = np.hstack((lstm_probs, xgb_probs_trimmed))  # Stack probabilities as features\n",
        "meta_y = y_val_lstm_labels  # True labels\n",
        "\n",
        "# ‚úÖ Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(meta_X, meta_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ‚úÖ Train Logistic Regression model\n",
        "meta_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
        "meta_model.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ Make final predictions using the meta-ensemble\n",
        "final_preds = meta_model.predict(X_test)\n",
        "\n",
        "# ‚úÖ Evaluate the meta-ensemble\n",
        "ensemble_accuracy = accuracy_score(y_test, final_preds)\n",
        "print(f\"‚úÖ Meta-Ensemble Accuracy: {ensemble_accuracy:.4%}\")\n",
        "\n",
        "# ‚úÖ Generate final predictions using the Meta-Ensemble\n",
        "final_preds = meta_model.predict(meta_X)\n",
        "\n",
        "# ‚úÖ Evaluate performance metrics\n",
        "ensemble_accuracy = accuracy_score(meta_y, final_preds)\n",
        "ensemble_cm = confusion_matrix(meta_y, final_preds)\n",
        "ensemble_report = classification_report(meta_y, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "\n",
        "# ‚úÖ Convert confusion matrix to DataFrame for visualization\n",
        "ensemble_cm_df = pd.DataFrame(ensemble_cm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                              columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Compute the confusion matrix as percentages\n",
        "ensemble_cm_percentage = ensemble_cm.astype(np.float32) / ensemble_cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "# ‚úÖ Convert to DataFrame for visualization\n",
        "ensemble_cm_percent_df = pd.DataFrame(ensemble_cm_percentage, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                                      columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# ‚úÖ Plot confusion matrix (Absolute Counts)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Final Meta-Ensemble Confusion Matrix (Counts)\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Plot confusion matrix (Percentages)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(ensemble_cm_percent_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"Final Meta-Ensemble Confusion Matrix (Percentages)\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Print classification report and accuracy\n",
        "print(f\"‚úÖ Final Meta-Ensemble Accuracy: {ensemble_accuracy:.4%}\")\n",
        "print(\"‚úÖ Final Meta-Ensemble Confusion Matrix:\\n\", ensemble_cm)\n",
        "print(\"‚úÖ Final Meta-Ensemble Classification Report:\\n\", ensemble_report)\n",
        "\n",
        "def compute_sens_spec_from_cm(cm, model_name=\"Logistic Meta-Ensemble\"):\n",
        "    metrics = []\n",
        "    for i, class_name in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "        TP = cm[i, i]\n",
        "        FN = cm[i, :].sum() - TP\n",
        "        FP = cm[:, i].sum() - TP\n",
        "        TN = cm.sum() - (TP + FN + FP)\n",
        "\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "\n",
        "        metrics.append({\n",
        "            \"Class\": class_name,\n",
        "            \"Sensitivity\": round(sensitivity, 4),\n",
        "            \"Specificity\": round(specificity, 4)\n",
        "        })\n",
        "    return pd.DataFrame(metrics)\n",
        "\n",
        "# Get classification metrics as a dict instead of a string\n",
        "ensemble_report_dict = classification_report(\n",
        "    meta_y, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"], output_dict=True\n",
        ")\n",
        "\n",
        "# Extract macro and weighted average F1 scores\n",
        "macro_f1 = ensemble_report_dict[\"macro avg\"][\"f1-score\"]\n",
        "weighted_f1 = ensemble_report_dict[\"weighted avg\"][\"f1-score\"]\n",
        "\n",
        "# Print the results\n",
        "print(f\"\\n‚úÖ Macro F1 Score: {macro_f1 * 100:.2f}%\")\n",
        "print(f\"‚úÖ Weighted F1 Score: {weighted_f1 * 100:.2f}%\")\n",
        "\n",
        "\n",
        "# ‚úÖ Compute and print per-class metrics FIRST\n",
        "ensemble_sens_spec = compute_sens_spec_from_cm(ensemble_cm)\n",
        "print(\"\\n‚úÖ Logistic Meta-Ensemble Sensitivity and Specificity:\")\n",
        "print(ensemble_sens_spec)\n",
        "\n",
        "# ‚úÖ Support values for each class\n",
        "support = {\n",
        "    \"Low\": (ensemble_cm[0, :].sum()),\n",
        "    \"Moderate\": (ensemble_cm[1, :].sum()),\n",
        "    \"Severe\": (ensemble_cm[2, :].sum())\n",
        "}\n",
        "\n",
        "# ‚úÖ Function to calculate overall metrics\n",
        "def compute_overall_sens_spec(metrics_df, support_dict):\n",
        "    total = sum(support_dict.values())\n",
        "    weighted_sens = sum(metrics_df.loc[i, \"Sensitivity\"] * support_dict[c] for i, c in enumerate(support_dict)) / total\n",
        "    weighted_spec = sum(metrics_df.loc[i, \"Specificity\"] * support_dict[c] for i, c in enumerate(support_dict)) / total\n",
        "    return round(weighted_sens * 100, 2), round(weighted_spec * 100, 2)\n",
        "\n",
        "# ‚úÖ Then calculate overall values\n",
        "overall_sens, overall_spec = compute_overall_sens_spec(ensemble_sens_spec, support)\n",
        "print(f\"\\n‚úÖ Overall Sensitivity: {overall_sens:.2f}%\")\n",
        "print(f\"‚úÖ Overall Specificity: {overall_spec:.2f}%\")\n",
        "\n",
        "\n",
        "# ‚úÖ Save the logistic regression meta-ensemble model\n",
        "joblib.dump(meta_model, \"final_meta_ensemble_model.pkl\")\n",
        "\n",
        "# ‚úÖ Download the saved model to your computer\n",
        "from google.colab import files\n",
        "files.download(\"final_meta_ensemble_model.pkl\")\n",
        "\n",
        "print(\"‚úÖ Final Meta-Ensemble Model is saved and ready for download.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U59cozC0RM2"
      },
      "source": [
        "More metrics for the Champion Ensemble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjxc-yWD6eZA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsmXAr5V6ejS"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPi6I66N0YC-"
      },
      "source": [
        "Created individual reports for the two models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5.1.6.1 Reload data for misclassification analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ‚úÖ Reload trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Reload validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# Ensure that X_val_lstm is in float32 format\n",
        "X_val_lstm = X_val_lstm.astype(np.float32)\n",
        "\n",
        "# ‚úÖ Reload XGBoost validation data with categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Generate LSTM Predictions\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)  # Probabilities for Low, Moderate, Severe\n",
        "lstm_preds = np.argmax(lstm_probs, axis=1)  # Convert to class labels\n",
        "\n",
        "# ‚úÖ Use already validated categorical risk predictions from XGBoost\n",
        "xgb_preds = val_xgb[\"Predicted_Risk\"].values\n",
        "\n",
        "# ‚úÖ Convert one-hot encoded y_val_lstm back to class labels\n",
        "y_val_lstm_labels = np.argmax(y_val_lstm, axis=1)\n",
        "\n",
        "print(\"‚úÖ Reloaded all necessary data for misclassification analysis.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KwAqHiu95F83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Z17c-ysiCv"
      },
      "outputs": [],
      "source": [
        "# Step 5.1.6.2 Misclassification rate comparison between airfields. XGBoost adjusted to match LSTM by removing the first 96 predictions, based on 4 days utilized by the LSTM.\n",
        "# ‚úÖ Step 1: Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ‚úÖ Step 2: Load XGBoost predictions and ground truth from CSV\n",
        "xgb_df = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "xgb_preds = xgb_df[\"Predicted_Risk\"].values\n",
        "xgb_true_labels = xgb_df[\"Actual_Risk\"].values\n",
        "\n",
        "# Ensure that X_val_lstm is in float32 format\n",
        "X_val_lstm = X_val_lstm.astype(np.float32)\n",
        "\n",
        "# ‚úÖ Get LSTM Predictions\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)  # Probabilities for Low, Moderate, Severe\n",
        "lstm_preds = np.argmax(lstm_probs, axis=1)  # Convert to class labels\n",
        "\n",
        "\n",
        "# ‚úÖ Step 4: Get the true labels used in LSTM validation (true y_val source!)\n",
        "if y_val_lstm.ndim == 2:\n",
        "    lstm_true_labels = np.argmax(y_val_lstm, axis=1)\n",
        "else:\n",
        "    lstm_true_labels = y_val_lstm.copy()\n",
        "\n",
        "# ‚úÖ Step 5: Sanity check - make sure LSTM metrics match model.evaluate\n",
        "print(\"\\n‚úÖ Classification report directly using LSTM validation set:\")\n",
        "print(classification_report(lstm_true_labels, lstm_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"]))\n",
        "\n",
        "# ‚úÖ Step 6: Align lengths just in case\n",
        "min_len = min(len(lstm_preds), len(xgb_preds), len(lstm_true_labels), len(xgb_true_labels))\n",
        "\n",
        "lstm_preds_trimmed = lstm_preds[:min_len]\n",
        "xgb_preds_trimmed = xgb_preds[:min_len]\n",
        "lstm_true_trimmed = lstm_true_labels[:min_len]\n",
        "xgb_true_trimmed = xgb_true_labels[:min_len]\n",
        "\n",
        "# ‚úÖ Step 7: Create DataFrames\n",
        "df_lstm = pd.DataFrame({\n",
        "    \"LSTM_Predicted_Risk\": lstm_preds_trimmed,\n",
        "    \"Actual_Risk_LSTM\": lstm_true_trimmed\n",
        "})\n",
        "df_xgb = pd.DataFrame({\n",
        "    \"XGBoost_Predicted_Risk\": xgb_preds_trimmed,\n",
        "    \"Actual_Risk_XGB\": xgb_true_trimmed\n",
        "})\n",
        "\n",
        "# ‚úÖ Step 8: Compare on same label reference (only if they're equal)\n",
        "if np.array_equal(lstm_true_trimmed, xgb_true_trimmed):\n",
        "    common_true_labels = lstm_true_trimmed\n",
        "    print(\"‚úÖ Ground truth labels are aligned. Proceeding with comparison.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Labels differ. Each model will be evaluated against its own ground truth.\")\n",
        "    common_true_labels = None  # for safety\n",
        "\n",
        "# ‚úÖ Step 9: Plot confusion matrices\n",
        "cm_lstm = confusion_matrix(lstm_true_trimmed, lstm_preds_trimmed)\n",
        "cm_xgb = confusion_matrix(xgb_true_trimmed, xgb_preds_trimmed)\n",
        "\n",
        "cm_lstm_df = pd.DataFrame(cm_lstm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                          columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "cm_xgb_df = pd.DataFrame(cm_xgb, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                         columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_lstm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"LSTM Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_xgb_df, annot=True, fmt=\"d\", cmap=\"Oranges\")\n",
        "plt.title(\"XGBoost Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Step 10: Misclassification Comparison (only if aligned)\n",
        "if common_true_labels is not None:\n",
        "    merged_df = pd.DataFrame({\n",
        "        \"Actual_Risk\": common_true_labels,\n",
        "        \"LSTM_Predicted_Risk\": lstm_preds_trimmed,\n",
        "        \"XGBoost_Predicted_Risk\": xgb_preds_trimmed\n",
        "    })\n",
        "    mismatch_df = merged_df[\n",
        "        (merged_df[\"LSTM_Predicted_Risk\"] != merged_df[\"Actual_Risk\"]) &\n",
        "        (merged_df[\"XGBoost_Predicted_Risk\"] != merged_df[\"Actual_Risk\"])\n",
        "    ]\n",
        "    print(\"‚úÖ Sample of Cases Where LSTM and XGBoost Made Different Errors:\\n\")\n",
        "    print(mismatch_df.head())\n",
        "\n",
        "# ‚úÖ Step 11: Individual Classification Reports\n",
        "print(\"\\n‚úÖ LSTM Classification Report:\\n\")\n",
        "print(classification_report(lstm_true_trimmed, lstm_preds_trimmed, target_names=[\"Low\", \"Moderate\", \"Severe\"]))\n",
        "\n",
        "print(\"\\n‚úÖ XGBoost Classification Report:\\n\")\n",
        "print(classification_report(xgb_true_trimmed, xgb_preds_trimmed, target_names=[\"Low\", \"Moderate\", \"Severe\"]))\n",
        "\n",
        "# ‚úÖ Compute F1 scores from classification reports\n",
        "lstm_report_dict = classification_report(\n",
        "    lstm_true_trimmed, lstm_preds_trimmed, target_names=[\"Low\", \"Moderate\", \"Severe\"], output_dict=True\n",
        ")\n",
        "xgb_report_dict = classification_report(\n",
        "    xgb_true_trimmed, xgb_preds_trimmed, target_names=[\"Low\", \"Moderate\", \"Severe\"], output_dict=True\n",
        ")\n",
        "\n",
        "lstm_macro_f1 = lstm_report_dict[\"macro avg\"][\"f1-score\"]\n",
        "lstm_weighted_f1 = lstm_report_dict[\"weighted avg\"][\"f1-score\"]\n",
        "\n",
        "xgb_macro_f1 = xgb_report_dict[\"macro avg\"][\"f1-score\"]\n",
        "xgb_weighted_f1 = xgb_report_dict[\"weighted avg\"][\"f1-score\"]\n",
        "\n",
        "print(f\"\\n‚úÖ LSTM Macro F1 Score: {lstm_macro_f1 * 100:.2f}%, Weighted F1 Score: {lstm_weighted_f1 * 100:.2f}%\")\n",
        "print(f\"‚úÖ XGBoost Macro F1 Score: {xgb_macro_f1 * 100:.2f}%, Weighted F1 Score: {xgb_weighted_f1 * 100:.2f}%\")\n",
        "\n",
        "\n",
        "# Step 11.1 Sensitivity and Specificity\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def compute_sens_spec(y_true, y_pred, model_name):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
        "    metrics = []\n",
        "\n",
        "    for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "        TP = cm[i, i]\n",
        "        FN = cm[i, :].sum() - TP\n",
        "        FP = cm[:, i].sum() - TP\n",
        "        TN = cm.sum() - (TP + FN + FP)\n",
        "\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "\n",
        "        metrics.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Class\": label,\n",
        "            \"Sensitivity\": round(sensitivity, 4),\n",
        "            \"Specificity\": round(specificity, 4)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(metrics)\n",
        "\n",
        "# ‚úÖ Calculate for both models\n",
        "df_lstm_metrics = compute_sens_spec(lstm_true_trimmed, lstm_preds_trimmed, \"LSTM\")\n",
        "df_xgb_metrics = compute_sens_spec(xgb_true_trimmed, xgb_preds_trimmed, \"XGBoost\")\n",
        "\n",
        "# ‚úÖ Combine and show\n",
        "df_all = pd.concat([df_lstm_metrics, df_xgb_metrics])\n",
        "print(\"\\n‚úÖ Sensitivity and Specificity Summary:\")\n",
        "print(df_all)\n",
        "\n",
        "# ‚úÖ Support values for each class\n",
        "def get_support(cm):\n",
        "    return {\n",
        "        \"Low\": cm[0, :].sum(),\n",
        "        \"Moderate\": cm[1, :].sum(),\n",
        "        \"Severe\": cm[2, :].sum()\n",
        "    }\n",
        "\n",
        "# ‚úÖ Function to compute overall weighted metrics\n",
        "def compute_overall_sens_spec(metrics_df, support_dict):\n",
        "    total = sum(support_dict.values())\n",
        "    weighted_sens = sum(metrics_df.loc[i, \"Sensitivity\"] * support_dict[c] for i, c in enumerate(support_dict)) / total\n",
        "    weighted_spec = sum(metrics_df.loc[i, \"Specificity\"] * support_dict[c] for i, c in enumerate(support_dict)) / total\n",
        "    return round(weighted_sens * 100, 2), round(weighted_spec * 100, 2)\n",
        "\n",
        "# ‚úÖ Get supports and calculate for both models\n",
        "lstm_support = get_support(confusion_matrix(lstm_true_trimmed, lstm_preds_trimmed))\n",
        "xgb_support = get_support(confusion_matrix(xgb_true_trimmed, xgb_preds_trimmed))\n",
        "\n",
        "lstm_overall_sens, lstm_overall_spec = compute_overall_sens_spec(df_lstm_metrics, lstm_support)\n",
        "xgb_overall_sens, xgb_overall_spec = compute_overall_sens_spec(df_xgb_metrics, xgb_support)\n",
        "\n",
        "# ‚úÖ Print results\n",
        "print(f\"\\n‚úÖ LSTM Overall Sensitivity: {lstm_overall_sens:.2f}%, Specificity: {lstm_overall_spec:.2f}%\")\n",
        "print(f\"‚úÖ XGBoost Overall Sensitivity: {xgb_overall_sens:.2f}%, Specificity: {xgb_overall_spec:.2f}%\")\n",
        "\n",
        "# ‚úÖ Step 12: Save for Ensemble Reuse\n",
        "np.save(\"lstm_preds_trimmed.npy\", lstm_preds_trimmed)\n",
        "np.save(\"xgb_preds_trimmed.npy\", xgb_preds_trimmed)\n",
        "np.save(\"lstm_true_trimmed.npy\", lstm_true_trimmed)\n",
        "np.save(\"xgb_true_trimmed.npy\", xgb_true_trimmed)\n",
        "\n",
        "files.download(\"lstm_preds_trimmed.npy\")\n",
        "files.download(\"xgb_preds_trimmed.npy\")\n",
        "files.download(\"lstm_true_trimmed.npy\")\n",
        "files.download(\"xgb_true_trimmed.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lk28wEh2VZn",
        "outputId": "0be2b97b-ad71-4875-91d6-9c1a21ee85b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Max_Risk_Quartile', 'Max_Risk_Quartile_Lag_2',\n",
            "       'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3',\n",
            "       'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2',\n",
            "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'Max_Risk',\n",
            "       'Predicted_Max_Risk', 'Actual_Risk', 'Predicted_Risk'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "#Step 6.1 Prepare for feature analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the XGBoost validation data (with categorical risk predictions)\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# Check the columns to ensure they match the features used for prediction\n",
        "print(val_xgb.columns)\n",
        "\n",
        "# Extract XGBoost features for prediction (ensure these match what was used during training)\n",
        "xgb_features = val_xgb[[\n",
        "    'Max_Risk_Quartile_Lag_2', 'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2',\n",
        "    'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2',\n",
        "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos'\n",
        "]].values\n",
        "\n",
        "# Ensure that the features are in float32 format\n",
        "xgb_features = xgb_features.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO-75NvqhSd7",
        "outputId": "2e399f6d-54c1-443a-969e-20e3c1ddf20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ XGBoost Features: ['Max_Risk_Quartile_Lag_2', 'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n",
            "‚úÖ LSTM Feature Count: 15\n",
            "‚ö†Ô∏è Feature count mismatch! Double-check preprocessing steps.\n"
          ]
        }
      ],
      "source": [
        "# Step 6.2 ‚úÖ Load XGBoost dataset\n",
        "val_xgb = pd.read_csv(\"val_XGBoost.csv\")\n",
        "\n",
        "# ‚úÖ Extract feature list for XGBoost (excluding target variable)\n",
        "xgb_features = val_xgb.drop(columns=['Max_Risk_Quartile']).columns.tolist()\n",
        "\n",
        "# ‚úÖ Load LSTM dataset and get feature count\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "lstm_feature_count = X_val_lstm.shape[2]  # Number of features in LSTM\n",
        "\n",
        "# ‚úÖ Print feature lists for comparison\n",
        "print(\"‚úÖ XGBoost Features:\", xgb_features)\n",
        "print(f\"‚úÖ LSTM Feature Count: {lstm_feature_count}\")\n",
        "\n",
        "# ‚úÖ Check if feature counts match\n",
        "if len(xgb_features) == lstm_feature_count:\n",
        "    print(\"‚úÖ LSTM and XGBoost feature counts match.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Feature count mismatch! Double-check preprocessing steps.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6.3, reload models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ‚úÖ Reload trained models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# ‚úÖ Reload validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# ‚úÖ Reload XGBoost validation data with categorical risk predictions\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Print confirmation\n",
        "print(\"‚úÖ Reloaded all necessary data for ROC Curve.\")\n"
      ],
      "metadata": {
        "id": "dm1J_gAtzmwH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6.4 Regeneate predictions for evaluation for Weighted averages only, do not use for Logarithmic Ensemble\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Reload models\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "xgb_model = joblib.load(\"xgb_tuned_quartile_model.pkl\")\n",
        "\n",
        "# Reload inputs\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "# Ensure that X_val_lstm is in float32 format\n",
        "X_val_lstm = X_val_lstm.astype(np.float32)\n",
        "# Generate predictions\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)\n",
        "xgb_probs = np.zeros((len(val_xgb), 3))\n",
        "for i, pred in enumerate(val_xgb[\"Predicted_Risk\"]):\n",
        "    xgb_probs[i, pred] = 1\n",
        "\n",
        "# Match lengths\n",
        "xgb_probs_trimmed = xgb_probs[-len(lstm_probs):]\n",
        "\n",
        "# Combine predictions\n",
        "ensemble_probs = (0.4 * lstm_probs) + (0.6 * xgb_probs_trimmed)\n",
        "final_preds = np.argmax(ensemble_probs, axis=1)\n"
      ],
      "metadata": {
        "id": "SSczHR5H237z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36TggVI23g4M"
      },
      "outputs": [],
      "source": [
        "#Step 7.1 Timeline\n",
        "# ‚úÖ Step 6: Graph - Actual vs. Predicted Risk Levels (Ensemble Model)\n",
        "#Step 6.1 Timeline\n",
        "# ‚úÖ Step 6: Graph - Actual vs. Predicted Risk Levels (Ensemble Model)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "# ‚úÖ Reload validation labels\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# ‚úÖ Reload saved ensemble model and extract predictions\n",
        "meta_model = joblib.load(\"final_meta_ensemble_model.pkl\")\n",
        "#final_preds = meta_model.predict(meta_X)  # Generate final predictions from the ensemble model\n",
        "\n",
        "\n",
        "# ‚úÖ Convert one-hot encoded y_val_lstm back to class labels\n",
        "y_true_class = np.argmax(y_val_lstm, axis=1)\n",
        "\n",
        "# ‚úÖ Ensure both arrays are the same length\n",
        "if len(y_true_class) != len(final_preds):\n",
        "    min_length = min(len(y_true_class), len(final_preds))\n",
        "    y_true_class = y_true_class[:min_length]\n",
        "    final_preds = final_preds[:min_length]\n",
        "\n",
        "# ‚úÖ Create DataFrame for visualization\n",
        "results_df = pd.DataFrame({\n",
        "    'Time Step': np.arange(len(y_true_class)),  # Sequential index\n",
        "    'Actual Risk Level': y_true_class,\n",
        "    'Predicted Risk Level': final_preds\n",
        "})\n",
        "\n",
        "# ‚úÖ Apply rolling mean for smoother trend visualization\n",
        "rolling_window = 50  # Adjust window size for more or less smoothing\n",
        "results_df['Actual Risk (Smoothed)'] = results_df['Actual Risk Level'].rolling(window=rolling_window, min_periods=1).mean()\n",
        "results_df['Predicted Risk (Smoothed)'] = results_df['Predicted Risk Level'].rolling(window=rolling_window, min_periods=1).mean()\n",
        "\n",
        "# ‚úÖ Plot the smoothed actual vs. predicted risk levels\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(results_df['Time Step'], results_df['Actual Risk (Smoothed)'], label='Actual Risk Level (Smoothed)', linewidth=2, color='blue')\n",
        "plt.plot(results_df['Time Step'], results_df['Predicted Risk (Smoothed)'], label='Predicted Risk Level (Smoothed)', linewidth=2, color='orange')\n",
        "\n",
        "# ‚úÖ Formatting and Labels\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Risk Level (0 = Low, 1 = Moderate, 2 = Severe)')\n",
        "plt.title('Actual vs Predicted Risk Levels - Ensemble Model')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# ‚úÖ Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpAunWWb1CW3",
        "outputId": "0a2f50c2-9df2-42b4-dc2d-c0a3a2680fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ XGBoost Features: ['Max_Risk_Quartile_Lag_2', 'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n",
            "‚úÖ LSTM Feature Count: 15\n",
            "‚ö†Ô∏è Feature count mismatch! Double-check preprocessing steps.\n"
          ]
        }
      ],
      "source": [
        "# Step 7.2 ‚úÖ Extract Feature Lists for LSTM and XGBoost\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load XGBoost dataset\n",
        "val_xgb = pd.read_csv(\"val_XGBoost.csv\")\n",
        "xgb_features = val_xgb.drop(columns=['Max_Risk_Quartile']).columns.tolist()\n",
        "\n",
        "# ‚úÖ Load LSTM dataset\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "lstm_feature_count = X_val_lstm.shape[2]  # Number of features in LSTM\n",
        "# Ensure that X_val_lstm is in float32 format\n",
        "X_val_lstm = X_val_lstm.astype(np.float32)\n",
        "\n",
        "print(\"‚úÖ XGBoost Features:\", xgb_features)\n",
        "print(f\"‚úÖ LSTM Feature Count: {lstm_feature_count}\")\n",
        "\n",
        "# Ensure feature alignment:\n",
        "if len(xgb_features) == lstm_feature_count:\n",
        "    print(\"‚úÖ LSTM and XGBoost feature counts match.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Feature count mismatch! Double-check preprocessing steps.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JB_2WoBh3kv",
        "outputId": "114722f9-fa34-4f5a-a315-7e72f58a5af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ XGBoost Features: ['Max_Risk_Quartile_Lag_2', 'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n",
            "‚úÖ LSTM Feature Count: 15\n",
            "‚ö†Ô∏è Feature count mismatch! Double-check preprocessing steps.\n"
          ]
        }
      ],
      "source": [
        "# Step 7.3 ‚úÖ Extract Feature Lists for LSTM and XGBoost\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load XGBoost dataset\n",
        "val_xgb = pd.read_csv(\"val_XGBoost.csv\")\n",
        "xgb_features = val_xgb.drop(columns=['Max_Risk_Quartile']).columns.tolist()\n",
        "\n",
        "# ‚úÖ Load LSTM dataset\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "lstm_feature_count = X_val_lstm.shape[2]  # Number of features in LSTM\n",
        "# Ensure that X_val_lstm is in float32 format\n",
        "X_val_lstm = X_val_lstm.astype(np.float32)\n",
        "\n",
        "print(\"‚úÖ XGBoost Features:\", xgb_features)\n",
        "print(f\"‚úÖ LSTM Feature Count: {lstm_feature_count}\")\n",
        "\n",
        "# Ensure feature alignment:\n",
        "if len(xgb_features) == lstm_feature_count:\n",
        "    print(\"‚úÖ LSTM and XGBoost feature counts match.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Feature count mismatch! Double-check preprocessing steps.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iit_YvW1kZEt"
      },
      "outputs": [],
      "source": [
        "# Step 7.4 ‚úÖ Trim XGBoost predictions to match LSTM sample count\n",
        "xgb_probs = xgb_probs[-8664:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HXujp00kbvM"
      },
      "outputs": [],
      "source": [
        "# Step 7.4.1 ROC from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Get predicted probabilities from the meta logistic model\n",
        "meta_probs = meta_model.predict_proba(meta_X)\n",
        "y_true = np.argmax(y_val_lstm, axis=1)  # True class labels (0,1,2)\n",
        "\n",
        "# ‚úÖ Plot ROC Curve for each class\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    fpr, tpr, _ = roc_curve(y_true == i, meta_probs[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{label} Risk (AUC = {roc_auc:.3f})\")\n",
        "\n",
        "# ‚úÖ Finalize plot\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Logistic Meta-Ensemble (MDW)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d31eYRaH6gRT"
      },
      "outputs": [],
      "source": [
        "# Step 7.4.1.1 Composite ROC with overall curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "meta_probs = meta_model.predict_proba(meta_X)\n",
        "\n",
        "# ‚úÖ Convert true labels into one-hot format for ROC computation\n",
        "y_true_bin = label_binarize(y_true, classes=[0, 1, 2])  # Convert to one-hot encoding\n",
        "\n",
        "# ‚úÖ Compute ROC curves for each class\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "\n",
        "for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], meta_probs[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# ‚úÖ Compute Macro-Averaged ROC (Average of Class AUCs)\n",
        "fpr_macro = np.linspace(0, 1, 100)\n",
        "tpr_macro = np.zeros_like(fpr_macro)\n",
        "for i in range(len(fpr)):\n",
        "    tpr_macro += np.interp(fpr_macro, fpr[i], tpr[i])\n",
        "tpr_macro /= len(fpr)  # Average across classes\n",
        "roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
        "\n",
        "# ‚úÖ Compute Micro-Averaged ROC (Flatten All Classes into One ROC)\n",
        "fpr_micro, tpr_micro, _ = roc_curve(y_true_bin.ravel(), ensemble_probs.ravel())\n",
        "roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "\n",
        "# ‚úÖ Plot ROC Curve for Each Class\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    plt.plot(fpr[i], tpr[i], label=f\"{label} Risk (AUC = {roc_auc[i]:.3f})\")\n",
        "\n",
        "# ‚úÖ Plot Macro- and Micro-Averaged ROC Curves\n",
        "plt.plot(fpr_macro, tpr_macro, label=f\"Macro Avg (AUC = {roc_auc_macro:.3f})\", linestyle='dashed')\n",
        "plt.plot(fpr_micro, tpr_micro, label=f\"Micro Avg (AUC = {roc_auc_micro:.3f})\", linestyle='dotted')\n",
        "\n",
        "# ‚úÖ Finalize ROC Curve Plot\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Final Meta-Ensemble Model with Composite Scores\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7.4.1.2 reload models\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Reload training data to get column names\n",
        "train_xgb = pd.read_csv(\"train_XGBoost.csv\")\n",
        "xgb_feature_names = [col for col in train_xgb.columns if col in xgb_model.feature_names_in_]\n",
        "\n",
        "# ‚úÖ Check the number of features stored in the XGBoost model\n",
        "num_xgb_features = len(xgb_model.feature_importances_)\n",
        "print(f\"‚úÖ XGBoost model was trained with {num_xgb_features} features.\")\n",
        "\n",
        "# ‚úÖ Check the extracted feature names count\n",
        "print(f\"‚úÖ Extracted {len(xgb_feature_names)} feature names.\")\n",
        "\n",
        "# ‚úÖ Print for debugging\n",
        "print(\"‚úÖ Feature names extracted:\", xgb_feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvTGh8XJ4U_m",
        "outputId": "b4f76d8f-7754-410e-f76d-24df07a475d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ XGBoost model was trained with 11 features.\n",
            "‚úÖ Extracted 11 feature names.\n",
            "‚úÖ Feature names extracted: ['Max_Risk_Quartile_Lag_2', 'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pksa3MtrHojw",
        "outputId": "0fd148e6-eb17-4982-8b4b-719efc99298b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ XGBoost feature count now matches!\n"
          ]
        }
      ],
      "source": [
        "# Step 7.4.3 ‚úÖ Trim the extracted feature list to match the number of features XGBoost actually used\n",
        "xgb_feature_names = xgb_feature_names[:num_xgb_features]\n",
        "\n",
        "# ‚úÖ Ensure feature count now matches\n",
        "if len(xgb_feature_names) == len(xgb_model.feature_importances_):\n",
        "    print(\"‚úÖ XGBoost feature count now matches!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Still a mismatch! Need to check preprocessing steps.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nON99iqNMMSE"
      },
      "outputs": [],
      "source": [
        "# Step 7.4.4 Check data\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "train_xgb = pd.read_csv(\"train_XGBoost.csv\")\n",
        "\n",
        "# Check if the lag features already exist\n",
        "print(train_xgb.head())  # Show the first few rows\n",
        "print(\"Columns in dataset:\", train_xgb.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLC_kxYJMkms"
      },
      "outputs": [],
      "source": [
        "#Step 8 XGBoost feature list\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load trained XGBoost model\n",
        "xgb_model_path = \"xgb_tuned_quartile_model.pkl\"\n",
        "xgb_tuned = joblib.load(xgb_model_path)\n",
        "\n",
        "# ‚úÖ Load the training data used for feature names\n",
        "train_xgb = pd.read_csv(\"train_XGBoost.csv\")\n",
        "\n",
        "# ‚úÖ Drop the target column to get only feature names\n",
        "xgb_feature_names = train_xgb.drop(columns=[\"Max_Risk_Quartile\"]).columns.tolist()\n",
        "\n",
        "# ‚úÖ Create feature importance DataFrame\n",
        "xgb_feature_importance = pd.DataFrame({\n",
        "    'Feature': xgb_feature_names,\n",
        "    'Importance_XGBoost': xgb_tuned.feature_importances_\n",
        "}).sort_values(by='Importance_XGBoost', ascending=False)\n",
        "\n",
        "# ‚úÖ Save to CSV\n",
        "xgb_feature_path = \"/content/XGBoost_Feature_Importance.csv\"\n",
        "xgb_feature_importance.to_csv(xgb_feature_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ XGBoost Feature Importance saved to: {xgb_feature_path}\")\n",
        "\n",
        "# ‚úÖ Download\n",
        "from google.colab import files\n",
        "files.download(xgb_feature_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBEhe1f3OZar"
      },
      "outputs": [],
      "source": [
        "# Step 8.1 LSTM-RNN Feature selection using a COLAB runtime\n",
        "# Custom Feature list through permutation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ‚úÖ File Paths\n",
        "model_path = \"LSTM_Trained_Model.keras\"\n",
        "\n",
        "# ‚úÖ Load the Trained LSTM Model\n",
        "model = load_model(model_path)\n",
        "\n",
        "# ‚úÖ Load validation data\n",
        "x_val_LSTM = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_LSTM = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# Ensure all elements in x_val_LSTM are numeric arrays\n",
        "x_val_LSTM_fixed = np.array(x_val_LSTM.tolist(), dtype=np.float64)\n",
        "\n",
        "# Convert y_val_LSTM to float32 (recommended for deep learning models)\n",
        "y_val_LSTM_fixed = y_val_LSTM.astype(\"float64\")\n",
        "\n",
        "# Verify the new data types\n",
        "print(f\"‚úÖ x_val_LSTM dtype fixed: {x_val_LSTM_fixed.dtype}, shape: {x_val_LSTM_fixed.shape}\")\n",
        "print(f\"‚úÖ y_val_LSTM dtype fixed: {y_val_LSTM_fixed.dtype}, shape: {y_val_LSTM_fixed.shape}\")\n",
        "\n",
        "# ‚úÖ Feature List\n",
        "feature_columns = ['Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2',\n",
        "                  'Altimeter_Lag_3', 'Wind_Direction_Lag_2', 'Risk_Low_Lag_2',\n",
        "                  'Risk_Low_Lag_24', 'Risk_Moderate_Lag_2', 'Risk_Moderate_Lag_24',\n",
        "                  'Risk_Severe_Lag_2', 'Risk_Severe_Lag_24', 'hour_sin', 'hour_cos',\n",
        "                  'month_sin', 'month_cos']\n",
        "\n",
        "\n",
        "# ‚úÖ Ensure correct number of features\n",
        "num_features = x_val_LSTM.shape[2]\n",
        "if len(feature_columns) != num_features:\n",
        "    raise ValueError(f\"Feature mismatch: Expected {num_features} features, got {len(feature_columns)}.\")\n",
        "\n",
        "# ‚úÖ Permutation Importance Function\n",
        "def custom_permutation_importance(model, X, y, metric=accuracy_score, feature_names=None, num_repeats=10):\n",
        "    baseline_score = metric(np.argmax(y, axis=1), np.argmax(model.predict(X), axis=1))\n",
        "    feature_importances = []\n",
        "\n",
        "    for col_idx in range(X.shape[2]):\n",
        "        scores = []\n",
        "        for _ in range(num_repeats):\n",
        "            X_permuted = X.copy()\n",
        "            np.random.shuffle(X_permuted[:, :, col_idx])  # Shuffle the entire column\n",
        "            permuted_score = metric(np.argmax(y, axis=1), np.argmax(model.predict(X_permuted), axis=1))\n",
        "            scores.append(baseline_score - permuted_score)\n",
        "\n",
        "        feature_importances.append(np.mean(scores))\n",
        "\n",
        "    return pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# ‚úÖ Compute Permutation Importance\n",
        "importance_df = custom_permutation_importance(\n",
        "    model, x_val_LSTM_fixed, y_val_LSTM_fixed, metric=accuracy_score, feature_names=feature_columns, num_repeats=10\n",
        ")\n",
        "\n",
        "\n",
        "# ‚úÖ Plot Feature Importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=importance_df, palette=\"viridis\")\n",
        "plt.title(\"Feature Importance (Permutation)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Save Results\n",
        "output_path = \"/content/LSTM_Feature_Importance.csv\"\n",
        "importance_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Feature importances saved to: {output_path}\")\n",
        "\n",
        "# ‚úÖ Download the file manually\n",
        "from google.colab import files\n",
        "files.download(output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tX1XCx03noj",
        "outputId": "87b2438c-6903-4c1e-89a0-e863b903e413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ x_val_LSTM dtype fixed: float64, shape: (8664, 96, 15)\n",
            "‚úÖ y_val_LSTM dtype fixed: float64, shape: (8664, 3)\n"
          ]
        }
      ],
      "source": [
        "STep 8.2, Check models\n",
        "import numpy as np\n",
        "\n",
        "# Ensure all elements in x_val_LSTM are numeric arrays\n",
        "x_val_LSTM_fixed = np.array(x_val_LSTM.tolist(), dtype=np.float64)\n",
        "\n",
        "# Convert y_val_LSTM to float32 (recommended for deep learning models)\n",
        "y_val_LSTM_fixed = y_val_LSTM.astype(\"float64\")\n",
        "\n",
        "# Verify the new data types\n",
        "print(f\"‚úÖ x_val_LSTM dtype fixed: {x_val_LSTM_fixed.dtype}, shape: {x_val_LSTM_fixed.shape}\")\n",
        "print(f\"‚úÖ y_val_LSTM dtype fixed: {y_val_LSTM_fixed.dtype}, shape: {y_val_LSTM_fixed.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt83FBR7DRNL"
      },
      "source": [
        "Combined the features lists from both and used the champion ensemble percentages to create a new champion model feature list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "63ONvfX0ljFG",
        "outputId": "5e6df7eb-d2c8-4bed-fdcb-cb49260f83dc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a5da68ae-3065-475c-a6f4-1d5b49fd719d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a5da68ae-3065-475c-a6f4-1d5b49fd719d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving IAH_LSTM_Preprocessed.csv to IAH_LSTM_Preprocessed.csv\n",
            "Saving IAH_Preprocessed_2.csv to IAH_Preprocessed_2.csv\n",
            "Saving IAH_XGBoost_Preprocessed.csv to IAH_XGBoost_Preprocessed.csv\n",
            "Saving LSTM_Trained_Model.keras to LSTM_Trained_Model.keras\n",
            "Saving train_XGBoost.csv to train_XGBoost.csv\n",
            "Saving val_XGBoost.csv to val_XGBoost.csv\n",
            "Saving X_train_LSTM.npy to X_train_LSTM.npy\n",
            "Saving X_val_LSTM.npy to X_val_LSTM.npy\n",
            "Saving xgb_tuned_quartile_model.pkl to xgb_tuned_quartile_model.pkl\n",
            "Saving y_train_LSTM.npy to y_train_LSTM.npy\n",
            "Saving y_val_LSTM.npy to y_val_LSTM.npy\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6jOVIOjMaZRF"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSmXncPn1rm2"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 9: Load Feature Importances and Logistic Meta-Ensemble Weights\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ‚úÖ Load XGBoost and LSTM Feature Importance CSVs\n",
        "xgb_importance_df = pd.read_csv(\"XGBoost_Feature_Importance.csv\")\n",
        "lstm_importance_df = pd.read_csv(\"LSTM_Feature_Importance.csv\")\n",
        "\n",
        "print(\"‚úÖ Successfully loaded both feature importance files!\")\n",
        "\n",
        "# ‚úÖ Merge the two for side-by-side comparison\n",
        "merged_importance_df = pd.merge(\n",
        "    xgb_importance_df, lstm_importance_df, on=\"Feature\", how=\"outer\"\n",
        ").fillna(0)\n",
        "\n",
        "# ‚úÖ Rename columns for clarity\n",
        "merged_importance_df.rename(columns={\n",
        "    \"Importance_XGBoost\": \"XGBoost Importance\",\n",
        "    \"Importance\": \"LSTM Importance\"\n",
        "}, inplace=True)\n",
        "\n",
        "# ‚úÖ Bar Chart: Compare XGBoost vs. LSTM Feature Importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(\n",
        "    data=merged_importance_df.melt(id_vars=\"Feature\", var_name=\"Model\", value_name=\"Importance\"),\n",
        "    x=\"Importance\", y=\"Feature\", hue=\"Model\", palette=[\"blue\", \"red\"]\n",
        ")\n",
        "plt.title(\"Feature Importance Comparison: XGBoost vs. LSTM\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.legend(title=\"Model\")\n",
        "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Save this merged file (even though it's not for the final ensemble)\n",
        "merged_importance_df[\"Average Base Importance\"] = (\n",
        "    merged_importance_df[\"XGBoost Importance\"] + merged_importance_df[\"LSTM Importance\"]\n",
        ") / 2\n",
        "\n",
        "# ‚úÖ Save merged base importance scores\n",
        "merged_path = \"/content/BaseModel_Feature_Importance.csv\"\n",
        "merged_importance_df.to_csv(merged_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Base Model Feature Importance saved to: {merged_path}\")\n",
        "\n",
        "# ‚úÖ Now extract feature importance from the Meta-Ensemble (Logistic Regression)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ‚úÖ Load trained logistic meta-model\n",
        "meta_model = joblib.load(\"final_meta_ensemble_model.pkl\")\n",
        "\n",
        "# ‚úÖ Define meta input features (probabilities from LSTM + XGBoost)\n",
        "meta_features = ['LSTM_Low', 'LSTM_Moderate', 'LSTM_Severe',\n",
        "                 'XGB_Low', 'XGB_Moderate', 'XGB_Severe']\n",
        "\n",
        "# ‚úÖ Extract weights from meta-model\n",
        "logistic_weights = pd.DataFrame(\n",
        "    meta_model.coef_.T,\n",
        "    index=meta_features,\n",
        "    columns=[\"Class_Low\", \"Class_Moderate\", \"Class_Severe\"]\n",
        ")\n",
        "\n",
        "# ‚úÖ Compute average absolute importance across all classes\n",
        "logistic_weights[\"Mean_Abs_Weight\"] = logistic_weights.abs().mean(axis=1)\n",
        "\n",
        "# ‚úÖ Sort and plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "logistic_weights.sort_values(\"Mean_Abs_Weight\", ascending=False)[\"Mean_Abs_Weight\"].plot(\n",
        "    kind=\"barh\", color=\"purple\", title=\"Meta-Ensemble (Logistic Regression) Feature Importance\"\n",
        ")\n",
        "plt.xlabel(\"Mean Absolute Weight\")\n",
        "plt.ylabel(\"Feature (LSTM/XGB Probabilities)\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Save logistic importance\n",
        "logistic_path = \"/content/Logistic_Meta_Feature_Importance.csv\"\n",
        "logistic_weights.to_csv(logistic_path)\n",
        "print(f\"‚úÖ Logistic Meta-Ensemble Feature Importance saved to: {logistic_path}\")\n",
        "\n",
        "# ‚úÖ Optionally, download all\n",
        "from google.colab import files\n",
        "files.download(merged_path)\n",
        "files.download(logistic_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3Wmev7wnI_U"
      },
      "outputs": [],
      "source": [
        "#Step 9.1 More in depth feature analysis\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# ‚úÖ Load XGBoost and LSTM Feature Importance Files\n",
        "xgb_importance_df = pd.read_csv(\"XGBoost_Feature_Importance.csv\")\n",
        "lstm_importance_df = pd.read_csv(\"LSTM_Feature_Importance.csv\")\n",
        "\n",
        "# ‚úÖ Merge the two files for comparison\n",
        "merged_importance_df = pd.merge(\n",
        "    xgb_importance_df, lstm_importance_df, on=\"Feature\", how=\"outer\"\n",
        ").fillna(0)\n",
        "\n",
        "# ‚úÖ Rename for clarity\n",
        "merged_importance_df.rename(columns={\n",
        "    \"Importance_XGBoost\": \"XGBoost Importance\",\n",
        "    \"Importance\": \"LSTM Importance\"\n",
        "}, inplace=True)\n",
        "\n",
        "# ‚úÖ Compute sorting metric: Max importance across both base models\n",
        "merged_importance_df[\"Max Importance\"] = merged_importance_df[[\"XGBoost Importance\", \"LSTM Importance\"]].max(axis=1)\n",
        "\n",
        "# ‚úÖ Sort by max importance\n",
        "merged_importance_df_sorted = merged_importance_df.sort_values(by=\"Max Importance\", ascending=False)\n",
        "\n",
        "# ‚úÖ Plot base model comparison\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(\n",
        "    data=merged_importance_df_sorted.melt(id_vars=\"Feature\", var_name=\"Model\", value_name=\"Importance\"),\n",
        "    x=\"Importance\", y=\"Feature\", hue=\"Model\", palette=[\"blue\", \"red\"]\n",
        ")\n",
        "plt.title(\"Base Feature Importance: XGBoost vs. LSTM (Sorted)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.legend(title=\"Model\")\n",
        "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Load trained Logistic Meta-Ensemble Model\n",
        "meta_model = joblib.load(\"final_meta_ensemble_model.pkl\")\n",
        "\n",
        "# ‚úÖ Define meta features (6-class probability inputs)\n",
        "meta_features = ['LSTM_Low', 'LSTM_Moderate', 'LSTM_Severe',\n",
        "                 'XGB_Low', 'XGB_Moderate', 'XGB_Severe']\n",
        "\n",
        "# ‚úÖ Extract logistic regression weights\n",
        "logistic_weights = pd.DataFrame(\n",
        "    meta_model.coef_.T,\n",
        "    index=meta_features,\n",
        "    columns=[\"Class_Low\", \"Class_Moderate\", \"Class_Severe\"]\n",
        ")\n",
        "\n",
        "# ‚úÖ Compute average weight across classes\n",
        "logistic_weights[\"Meta Importance\"] = logistic_weights.abs().mean(axis=1)\n",
        "\n",
        "# ‚úÖ Sort and plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "logistic_weights.sort_values(\"Meta Importance\", ascending=False)[\"Meta Importance\"].plot(\n",
        "    kind=\"barh\", color=\"purple\", title=\"Meta-Ensemble Feature Importance (Logistic Coefficients)\"\n",
        ")\n",
        "plt.xlabel(\"Mean Absolute Weight\")\n",
        "plt.ylabel(\"Meta-Level Feature\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Save logistic weights and merged base model importances\n",
        "logistic_path = \"Logistic_Meta_Ensemble_Feature_Importance.csv\"\n",
        "logistic_weights.to_csv(logistic_path)\n",
        "\n",
        "ensemble_feature_path = \"Ensemble_Feature_Importance_Sorted.csv\"\n",
        "merged_importance_df_sorted.to_csv(ensemble_feature_path, index=False)\n",
        "\n",
        "# ‚úÖ Download results\n",
        "import google.colab.files\n",
        "google.colab.files.download(ensemble_feature_path)\n",
        "google.colab.files.download(logistic_path)\n",
        "\n",
        "# ‚úÖ Print confirmation\n",
        "print(\"\\n‚úÖ Base Model Feature Importance Table (Sorted):\")\n",
        "print(merged_importance_df_sorted.to_string(index=False))\n",
        "print(f\"\\n‚úÖ Logistic Meta-Ensemble Feature Importance saved to: {logistic_path}\")\n",
        "print(f\"‚úÖ Base Model Importance saved to: {ensemble_feature_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZU-kxK_orFi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cERS2XqouRi"
      },
      "source": [
        "Utilized bootstrapping the two individual models with 1000 iterations to find the confidence interval of each individual model, then used the same ensemble method to determine the final confidence interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yJGEntdVo8Qi"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41VKo0fwuT6s"
      },
      "outputs": [],
      "source": [
        "#Step 10 Load models for CI calculations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load XGBoost validation data\n",
        "val_xgb = pd.read_csv(\"val_XGBoost.csv\")\n",
        "\n",
        "# ‚úÖ Extract feature columns (Ensure target 'Max_Risk_Quartile' is NOT included)\n",
        "X_val_XGB = val_xgb.drop(columns=['Max_Risk_Quartile']).values  # Convert to NumPy array\n",
        "\n",
        "# ‚úÖ Save as `.npy` for compatibility with bootstrapping\n",
        "np.save(\"X_val_XGB.npy\", X_val_XGB)\n",
        "\n",
        "print(\"‚úÖ XGBoost validation data saved as X_val_XGB.npy!\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"X_val_XGB.npy\")\n",
        "print(\"‚úÖ XGBoost validation data downloaded as X_val_XGB.npy!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10.1 add date and time to required files\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load source file with timestamp\n",
        "preprocessed_df = pd.read_csv(\"SEA_XGBoost_Preprocessed.csv\", parse_dates=[\"DATE_and_Time\"])\n",
        "\n",
        "# ‚úÖ Load current categorical prediction file (missing date)\n",
        "val_current = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Sanity check: match row counts\n",
        "assert len(preprocessed_df) >= len(val_current), \"‚ùå Not enough rows in preprocessed file!\"\n",
        "\n",
        "# ‚úÖ Extract the last N timestamps from the preprocessed file\n",
        "# This assumes that validation data was the last portion of the full dataset (common in time-series)\n",
        "val_timestamps = preprocessed_df[\"DATE_and_Time\"].iloc[-len(val_current):].reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ Attach timestamp column to current prediction file\n",
        "val_current[\"DATE_and_Time\"] = val_timestamps\n",
        "\n",
        "# ‚úÖ Save to new file\n",
        "output_path = \"val_XGBoost_with_Categorical_Risk_with_date.csv\"\n",
        "val_current.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ DATE_and_Time successfully attached and saved to: {output_path}\")\n",
        "print(val_current[[\"DATE_and_Time\", \"Predicted_Risk\"]].head())"
      ],
      "metadata": {
        "id": "vGBkxQV-gCYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 10.1.1 Continue adding date and times to files\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load source file with timestamp\n",
        "preprocessed_df = pd.read_csv(\"SEA_XGBoost_Preprocessed.csv\", parse_dates=[\"DATE_and_Time\"])\n",
        "\n",
        "# ‚úÖ Load current categorical prediction file (missing date)\n",
        "val_current = pd.read_csv(\"val_XGBoost_with_Categorical_Risk.csv\")\n",
        "\n",
        "# ‚úÖ Sanity check: match row counts\n",
        "assert len(preprocessed_df) >= len(val_current), \"‚ùå Not enough rows in preprocessed file!\"\n",
        "\n",
        "# ‚úÖ Extract the last N timestamps from the preprocessed file\n",
        "# This assumes that validation data was the last portion of the full dataset (common in time-series)\n",
        "val_timestamps = preprocessed_df[\"DATE_and_Time\"].iloc[-len(val_current):].reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ Attach timestamp column to current prediction file\n",
        "val_current[\"DATE_and_Time\"] = val_timestamps\n",
        "\n",
        "# ‚úÖ Save to new file\n",
        "output_path = \"val_XGBoost_with_Categorical_Risk_with_date.csv\"\n",
        "val_current.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ DATE_and_Time successfully attached and saved to: {output_path}\")\n",
        "print(val_current[[\"DATE_and_Time\", \"Predicted_Risk\"]].head())\n"
      ],
      "metadata": {
        "id": "yj4T4wvdZq4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10.1.2 ‚úÖ Provide download link\n",
        "import google.colab.files\n",
        "google.colab.files.download(\"val_XGBoost_with_Categorical_Risk_with_date.csv\")"
      ],
      "metadata": {
        "id": "ZvTddN1Fb4Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg4-SPzzoqbC"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 10.2: Bootstrap Confidence Intervals for Logistic Meta-Ensemble and Base Models\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import google.colab.files\n",
        "\n",
        "# ‚úÖ Load meta-model and base models\n",
        "meta_model = joblib.load(\"final_meta_ensemble_model.pkl\")\n",
        "lstm_model = load_model(\"LSTM_Trained_Model.keras\")\n",
        "\n",
        "# ‚úÖ Load validation data\n",
        "X_val_lstm = np.load(\"X_val_LSTM.npy\", allow_pickle=True).astype(np.float32)\n",
        "lstm_probs = lstm_model.predict(X_val_lstm)\n",
        "\n",
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk_with_date.csv\", parse_dates=[\"DATE_and_Time\"])\n",
        "xgb_preds = val_xgb[\"Predicted_Risk\"].values\n",
        "\n",
        "# ‚úÖ Construct one-hot XGBoost probabilities\n",
        "xgb_probs = np.zeros((len(xgb_preds), 3))\n",
        "for i, pred in enumerate(xgb_preds):\n",
        "    xgb_probs[i, pred] = 1\n",
        "\n",
        "# ‚úÖ Align lengths\n",
        "min_len = min(len(lstm_probs), len(xgb_probs))\n",
        "lstm_probs = lstm_probs[-min_len:]\n",
        "xgb_probs = xgb_probs[-min_len:]\n",
        "meta_X = np.hstack((lstm_probs, xgb_probs))\n",
        "\n",
        "# ‚úÖ Load and align metadata\n",
        "raw_df = pd.read_excel(\"SEA_Data_Python_Imputed.xlsx\", parse_dates=[\"DATE_and_Time\"])\n",
        "val_timestamps = val_xgb[\"DATE_and_Time\"][-min_len:]\n",
        "meta_metadata = raw_df[raw_df[\"DATE_and_Time\"].isin(val_timestamps)].sort_values(\"DATE_and_Time\").reset_index(drop=True)\n",
        "meta_metadata = meta_metadata[-min_len:]\n",
        "assert len(meta_metadata) == min_len, \"‚ùå Metadata length mismatch!\"\n",
        "\n",
        "# ‚úÖ Bootstrap parameters\n",
        "n_bootstraps = 1000\n",
        "z_score = 1.96\n",
        "\n",
        "# ‚úÖ Reusable function for bootstrapped CI\n",
        "def bootstrap_ci(probs, label, n_bootstraps=1000, z_score=1.96):\n",
        "    n_samples = probs.shape[0]\n",
        "    sum_preds = np.zeros_like(probs)\n",
        "    sum_preds_sq = np.zeros_like(probs)\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        sample = probs[indices]\n",
        "        sum_preds += sample\n",
        "        sum_preds_sq += sample ** 2\n",
        "\n",
        "    mean_preds = sum_preds / n_bootstraps\n",
        "    std_preds = np.sqrt((sum_preds_sq / n_bootstraps) - (mean_preds ** 2))\n",
        "    lower = np.clip(mean_preds - z_score * std_preds, 0, 1)\n",
        "    upper = np.clip(mean_preds + z_score * std_preds, 0, 1)\n",
        "    width = upper - lower\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        f\"Mean Prediction ({label} Low)\": mean_preds[:, 0],\n",
        "        f\"Lower Bound ({label} Low 95% CI)\": lower[:, 0],\n",
        "        f\"Upper Bound ({label} Low 95% CI)\": upper[:, 0],\n",
        "        f\"{label} Low CI Width\": width[:, 0],\n",
        "\n",
        "        f\"Mean Prediction ({label} Moderate)\": mean_preds[:, 1],\n",
        "        f\"Lower Bound ({label} Moderate 95% CI)\": lower[:, 1],\n",
        "        f\"Upper Bound ({label} Moderate 95% CI)\": upper[:, 1],\n",
        "        f\"{label} Moderate CI Width\": width[:, 1],\n",
        "\n",
        "        f\"Mean Prediction ({label} Severe)\": mean_preds[:, 2],\n",
        "        f\"Lower Bound ({label} Severe 95% CI)\": lower[:, 2],\n",
        "        f\"Upper Bound ({label} Severe 95% CI)\": upper[:, 2],\n",
        "        f\"{label} Severe CI Width\": width[:, 2]\n",
        "    })\n",
        "\n",
        "# ‚úÖ Run CI bootstrap for all models\n",
        "ensemble_probs = meta_model.predict_proba(meta_X)\n",
        "ci_ensemble = bootstrap_ci(ensemble_probs, \"Ensemble\")\n",
        "ci_lstm = bootstrap_ci(lstm_probs, \"LSTM\")\n",
        "ci_xgb = bootstrap_ci(xgb_probs, \"XGBoost\")\n",
        "\n",
        "# ‚úÖ Merge all CI results with metadata\n",
        "ci_full = pd.concat([\n",
        "    meta_metadata.reset_index(drop=True),\n",
        "    ci_ensemble.reset_index(drop=True),\n",
        "    ci_lstm.reset_index(drop=True),\n",
        "    ci_xgb.reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "# ‚úÖ Save full report\n",
        "save_path = \"Full_CI_with_LSTM_XGB_Ensemble.csv\"\n",
        "ci_full.to_csv(save_path, index=False)\n",
        "print(f\"‚úÖ Full CI report saved to: {save_path}\")\n",
        "google.colab.files.download(save_path)\n",
        "\n",
        "# ‚úÖ Print average CI widths\n",
        "print(\"\\nüîç Avg CI Widths\")\n",
        "print(\"Ensemble ‚Äî Low:\", ci_ensemble['Ensemble Low CI Width'].mean(),\n",
        "      \"| Moderate:\", ci_ensemble['Ensemble Moderate CI Width'].mean(),\n",
        "      \"| Severe:\", ci_ensemble['Ensemble Severe CI Width'].mean())\n",
        "print(\"LSTM     ‚Äî Low:\", ci_lstm['LSTM Low CI Width'].mean(),\n",
        "      \"| Moderate:\", ci_lstm['LSTM Moderate CI Width'].mean(),\n",
        "      \"| Severe:\", ci_lstm['LSTM Severe CI Width'].mean())\n",
        "print(\"XGBoost  ‚Äî Low:\", ci_xgb['XGBoost Low CI Width'].mean(),\n",
        "      \"| Moderate:\", ci_xgb['XGBoost Moderate CI Width'].mean(),\n",
        "      \"| Severe:\", ci_xgb['XGBoost Severe CI Width'].mean())\n",
        "\n",
        "\n",
        "# ‚úÖ Plot Severe Risk CI\n",
        "# ‚úÖ Plot Severe Risk CI from ci_ensemble\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ci_ensemble[\"Mean Prediction (Ensemble Severe)\"], label=\"Severe Risk (Mean)\", color=\"red\")\n",
        "plt.fill_between(range(min_len),\n",
        "                 ci_ensemble[\"Lower Bound (Ensemble Severe 95% CI)\"],\n",
        "                 ci_ensemble[\"Upper Bound (Ensemble Severe 95% CI)\"],\n",
        "                 alpha=0.2, color=\"red\")\n",
        "plt.title(\"Bootstrapped Confidence Interval - Severe Risk (Logistic Meta-Ensemble)\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Predicted Probability\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Plot Moderate Risk CI\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ci_ensemble[\"Mean Prediction (Ensemble Moderate)\"], label=\"Moderate Risk (Mean)\", color=\"blue\")\n",
        "plt.fill_between(range(min_len),\n",
        "                 ci_ensemble[\"Lower Bound (Ensemble Moderate 95% CI)\"],\n",
        "                 ci_ensemble[\"Upper Bound (Ensemble Moderate 95% CI)\"],\n",
        "                 alpha=0.2, color=\"blue\")\n",
        "plt.title(\"Bootstrapped Confidence Interval - Moderate Risk (Logistic Meta-Ensemble)\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Predicted Probability\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Plot Low Risk CI\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ci_ensemble[\"Mean Prediction (Ensemble Low)\"], label=\"Low Risk (Mean)\", color=\"green\")\n",
        "plt.fill_between(range(min_len),\n",
        "                 ci_ensemble[\"Lower Bound (Ensemble Low 95% CI)\"],\n",
        "                 ci_ensemble[\"Upper Bound (Ensemble Low 95% CI)\"],\n",
        "                 alpha=0.2, color=\"green\")\n",
        "plt.title(\"Bootstrapped Confidence Interval - Low Risk (Logistic Meta-Ensemble)\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Predicted Probability\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ‚úÖ Download (if using Colab)\n",
        "import google.colab.files\n",
        "google.colab.files.download(save_path)\n",
        "\n",
        "print(\"‚úÖ Ensemble 2 CI bootstrapping complete!\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO6Ep0Trd2mSY19K1fuQilB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}