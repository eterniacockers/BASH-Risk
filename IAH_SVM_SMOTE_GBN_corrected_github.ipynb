{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eterniacockers/BASH-Risk/blob/main/IAH_SVM_SMOTE_GBN_corrected_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r4PItpAxf4p"
      },
      "source": [
        "Start by uploading the imputed data file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhM7i5c4xlDB"
      },
      "source": [
        "Load LSTM data for testing a GBN."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1.4.1 load data files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Sa4uNIEEfo-Q",
        "outputId": "88338ea0-5652-4325-f390-9eb755d42b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7abaa095-ca1a-4217-9d42-37b2e080d8b2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7abaa095-ca1a-4217-9d42-37b2e080d8b2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Full_CI_with_LSTM_XGB_Ensemble.csv to Full_CI_with_LSTM_XGB_Ensemble.csv\n",
            "Saving IAH_LSTM_Preprocessed.csv to IAH_LSTM_Preprocessed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have to resample since none of the resmapled files saved properly. This will ensure the GBN runs on balanced data."
      ],
      "metadata": {
        "id": "RtnMbO3zTIEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 2: Split into Training and Validation Sets (Colab Version)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# ‚úÖ Load the preprocessed datasets\n",
        "lstm_file_path = \"IAH_LSTM_Preprocessed.csv\"\n",
        "\n",
        "df_lstm = pd.read_csv(lstm_file_path)\n",
        "\n",
        "\n",
        "# ‚úÖ Convert DATE_and_Time to datetime (if not already)\n",
        "df_lstm['DATE_and_Time'] = pd.to_datetime(df_lstm['DATE_and_Time'], errors='coerce')\n",
        "\n",
        "\n",
        "# ‚úÖ Ensure Risk_Category exists in LSTM dataset\n",
        "if 'Risk_Category' not in df_lstm.columns:\n",
        "    raise ValueError(\"‚ùå Risk_Category column is missing in LSTM dataset. Check preprocessing steps.\")\n",
        "\n",
        "\n",
        "# ‚úÖ Split datasets into training (2015‚Äì2018) and validation (2019)\n",
        "train_lstm = df_lstm[df_lstm['DATE_and_Time'].dt.year < 2019].reset_index(drop=True)\n",
        "val_lstm = df_lstm[df_lstm['DATE_and_Time'].dt.year == 2019].reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ Drop DATE_and_Time column after splitting\n",
        "train_lstm.drop(columns=['DATE_and_Time'], inplace=True)\n",
        "val_lstm.drop(columns=['DATE_and_Time'], inplace=True)\n",
        "\n",
        "\n",
        "# ‚úÖ Define function to create sequences for LSTM\n",
        "def create_sequences(data, target, sequence_length=96):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        x = data.iloc[i:(i + sequence_length)].values  # Feature sequence\n",
        "        y = target.iloc[i + sequence_length]  # Corresponding target\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# ‚úÖ Define sequence length for LSTM (96 time steps = 4 days)\n",
        "sequence_length = 96\n",
        "\n",
        "# ‚úÖ Convert Risk_Category to integer before encoding for LSTM\n",
        "train_lstm['Risk_Category'] = train_lstm['Risk_Category'].astype(int)\n",
        "val_lstm['Risk_Category'] = val_lstm['Risk_Category'].astype(int)\n",
        "\n",
        "# ‚úÖ Separate features and target\n",
        "features_train_lstm = train_lstm.drop(columns=['Risk_Category'])\n",
        "target_train_lstm = train_lstm['Risk_Category']\n",
        "features_val_lstm = val_lstm.drop(columns=['Risk_Category'])\n",
        "target_val_lstm = val_lstm['Risk_Category']\n",
        "\n",
        "# ‚úÖ Create sequences for LSTM training and validation\n",
        "X_train_lstm, y_train_lstm = create_sequences(features_train_lstm, target_train_lstm, sequence_length)\n",
        "X_val_lstm, y_val_lstm = create_sequences(features_val_lstm, target_val_lstm, sequence_length)\n",
        "\n",
        "# ‚úÖ One-hot encode target variables for LSTM classification\n",
        "num_classes = 3  # Explicitly set to 3 classes (Low, Moderate, Severe)\n",
        "y_train_lstm = to_categorical(y_train_lstm, num_classes=num_classes)\n",
        "y_val_lstm = to_categorical(y_val_lstm, num_classes=num_classes)\n",
        "\n",
        "# ‚úÖ Save processed LSTM data in Colab\n",
        "np.save(\"X_train_LSTM.npy\", X_train_lstm)\n",
        "np.save(\"y_train_LSTM.npy\", y_train_lstm)\n",
        "np.save(\"X_val_LSTM.npy\", X_val_lstm)\n",
        "np.save(\"y_val_LSTM.npy\", y_val_lstm)\n",
        "\n",
        "\n",
        "# ‚úÖ Print shapes for verification\n",
        "print(f\"‚úÖ LSTM - X_train shape: {X_train_lstm.shape}, y_train shape: {y_train_lstm.shape}\")\n",
        "print(f\"‚úÖ LSTM - X_val shape: {X_val_lstm.shape}, y_val shape: {y_val_lstm.shape}\")\n",
        "\n",
        "# download file\n",
        "files.download(\"X_train_LSTM.npy\")\n",
        "files.download(\"y_train_LSTM.npy\")\n",
        "files.download(\"X_val_LSTM.npy\")\n",
        "files.download(\"y_val_LSTM.npy\")"
      ],
      "metadata": {
        "id": "wekGV2I_NEf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c2ed7b9f-eece-4bf3-f127-2936ecac9bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LSTM - X_train shape: (34852, 96, 15), y_train shape: (34852, 3)\n",
            "‚úÖ LSTM - X_val shape: (8664, 96, 15), y_val shape: (8664, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f2558087-7945-4645-8daf-672e6df1629b\", \"X_train_LSTM.npy\", 291224536)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2b4db71a-dbf6-47dd-8385-0ebe9d114581\", \"y_train_LSTM.npy\", 836576)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0ae2b0e8-6a2a-4872-a38c-15029281cf23\", \"X_val_LSTM.npy\", 72396898)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e8ce6024-aca9-4f8f-b675-91374d71461d\", \"y_val_LSTM.npy\", 208064)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 3: SVM-SMOTE for Target Balancing in LSTM (Colab Version)\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import collections\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ‚úÖ Load processed training data\n",
        "# ‚úÖ Load processed training data with `allow_pickle=True`\n",
        "X_train = np.load(\"X_train_LSTM.npy\", allow_pickle=True)\n",
        "y_train = np.load(\"y_train_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "\n",
        "# ‚úÖ Flatten training features for SMOTE (Convert 3D ‚Üí 2D)\n",
        "train_features = X_train.reshape(X_train.shape[0], -1)  # Reshape to 2D for SMOTE\n",
        "train_target = np.argmax(y_train, axis=1)  # Convert one-hot encoding to class labels\n",
        "\n",
        "# ‚úÖ Check class distribution before balancing\n",
        "class_counts_before = collections.Counter(train_target)\n",
        "print(\"üìä Class distribution before SVM-SMOTE:\", class_counts_before)\n",
        "\n",
        "# ‚úÖ Plot class distribution before SMOTE\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=train_target, palette=\"viridis\", hue=train_target, dodge=False)\n",
        "plt.title(\"Class Distribution Before SVM-SMOTE\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(ticks=[0, 1, 2], labels=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "plt.legend([], [], frameon=False)\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Apply SVM-SMOTE to **only oversample** the minority classes\n",
        "svm_smote = SVMSMOTE(sampling_strategy='auto', random_state=42)  # Auto ensures **only minority class upsampling**\n",
        "train_features_resampled, train_target_resampled = svm_smote.fit_resample(train_features, train_target)\n",
        "\n",
        "# ‚úÖ Check class distribution after SVM-SMOTE\n",
        "class_counts_after = collections.Counter(train_target_resampled)\n",
        "print(\"üìä Class distribution after SVM-SMOTE:\", class_counts_after)\n",
        "\n",
        "# ‚úÖ Plot class distribution after SVM-SMOTE\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=train_target_resampled, palette=\"viridis\", hue=train_target_resampled, dodge=False)\n",
        "plt.title(\"Class Distribution After SVM-SMOTE\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(ticks=[0, 1, 2], labels=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "plt.legend([], [], frameon=False)\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Get sequence length and feature count (Reshape back to 3D for LSTM)\n",
        "sequence_length = X_train.shape[1]  # 96 time steps\n",
        "num_features = X_train.shape[2]  # Number of features per timestep\n",
        "\n",
        "train_features_resampled = train_features_resampled.reshape(-1, sequence_length, num_features)\n",
        "\n",
        "# ‚úÖ One-hot encode the resampled targets AFTER verifying balance\n",
        "train_target_resampled = to_categorical(train_target_resampled, num_classes=3)\n",
        "\n",
        "resample_method = \"SVM_SMOTE\"\n",
        "np.save(f\"X_train_{resample_method}.npy\", train_features_resampled)\n",
        "np.save(f\"y_train_{resample_method}.npy\", train_target_resampled)\n",
        "\n",
        "files.download(f\"X_train_{resample_method}.npy\")\n",
        "files.download(f\"y_train_{resample_method}.npy\")\n"
      ],
      "metadata": {
        "id": "PRGJYGH6M2VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2D for GBN\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Load unbalanced validation data\n",
        "X_val = np.load(\"X_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "\n",
        "# ‚úÖ Load resampled training data (SVM-SMOTE in this case)\n",
        "X_train = np.load(\"X_train_SVM_SMOTE.npy\")\n",
        "y_train = np.load(\"y_train_SVM_SMOTE.npy\")\n",
        "\n",
        "# ‚úÖ Flatten for GBN input\n",
        "# ‚úÖ Correct: take only the final time step (t = -1)\n",
        "X_train_flat = X_train[:, -1, :]  # shape: (n_samples, 15)\n",
        "X_val_flat = X_val[:, -1, :]      # shape: (n_samples, 15)\n",
        "\n",
        "\n",
        "y_train_flat = np.argmax(y_train, axis=1)\n",
        "y_val_flat = np.argmax(y_val, axis=1)\n",
        "\n",
        "# ‚úÖ Define feature names (if using 15 original features)\n",
        "feature_names = [\n",
        "    'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2',\n",
        "    'Risk_Low_Lag_2', 'Risk_Low_Lag_24', 'Risk_Moderate_Lag_2', 'Risk_Moderate_Lag_24',\n",
        "    'Risk_Severe_Lag_2', 'Risk_Severe_Lag_24', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos'\n",
        "]\n",
        "\n",
        "# ‚úÖ Map class index to label name\n",
        "label_map = {0: \"Low\", 1: \"Moderate\", 2: \"Severe\"}\n",
        "y_train_labels = pd.Series(y_train_flat).map(label_map)\n",
        "y_val_labels = pd.Series(y_val_flat).map(label_map)\n",
        "\n",
        "# ‚úÖ Rebuild DataFrames\n",
        "train_df = pd.DataFrame(X_train_flat, columns=feature_names)\n",
        "train_df[\"Risk_Category\"] = y_train_labels\n",
        "train_df[\"Risk_Category_Encoded\"] = y_train_flat\n",
        "\n",
        "val_df = pd.DataFrame(X_val_flat, columns=feature_names)\n",
        "val_df[\"Risk_Category\"] = y_val_labels\n",
        "val_df[\"Risk_Category_Encoded\"] = y_val_flat\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mrMrlk9pGC7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyZLzt4KzhIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33,
          "referenced_widgets": [
            "ce777994d7db4fa09a6b339c346d41fb",
            "ad929a87e2f04010bfdff549bdb99e9e"
          ]
        },
        "outputId": "7858edc3-d7e0-410e-979c-62d82253edb3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce777994d7db4fa09a6b339c346d41fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ‚úÖ Step 4: Optimized Bayesian Bi-LSTM Model Training for Classification with LSTM Ensemble files (Colab Version) adjusted weight\n",
        "# ‚úÖ Import modeling and evaluation libraries\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define features and target\n",
        "feature_cols = feature_names\n",
        "X = train_df[feature_cols].values\n",
        "y = train_df[\"Risk_Category_Encoded\"].values\n",
        "\n",
        "X_val_input = val_df[feature_cols].values\n",
        "y_val_input = val_df[\"Risk_Category_Encoded\"].values\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_val_scaled = scaler.transform(X_val_input)\n",
        "\n",
        "# Train PyMC model\n",
        "with pm.Model() as model:\n",
        "    coeffs = pm.Normal(\"coeffs\", mu=0, sigma=1, shape=(X_scaled.shape[1], 3))\n",
        "    intercept = pm.Normal(\"intercept\", mu=0, sigma=1, shape=(3,))\n",
        "    logits = pm.math.dot(X_scaled, coeffs) + intercept\n",
        "    y_obs = pm.Categorical(\"y_obs\", logit_p=logits, observed=y)\n",
        "    approx = pm.fit(method=\"advi\", n=50000)\n",
        "    trace = approx.sample(draws=1000)\n",
        "\n",
        "# Posterior prediction without aesara\n",
        "coeff_samples = trace.posterior[\"coeffs\"].stack(sample=(\"chain\", \"draw\")).values\n",
        "intercept_samples = trace.posterior[\"intercept\"].stack(sample=(\"chain\", \"draw\")).values\n",
        "coeff_samples = np.transpose(coeff_samples, (2, 0, 1))\n",
        "intercept_samples = np.transpose(intercept_samples, (1, 0))\n",
        "logits = np.einsum(\"sfc,nf->snc\", coeff_samples, X_val_scaled) + intercept_samples[:, np.newaxis, :]\n",
        "exp_logits = np.exp(logits - logits.max(axis=2, keepdims=True))\n",
        "probs = exp_logits / exp_logits.sum(axis=2, keepdims=True)\n",
        "mean_probs = probs.mean(axis=0)\n",
        "y_pred = np.argmax(mean_probs, axis=1)\n",
        "\n",
        "# Save predictions\n",
        "val_df[\"PyMC_Predicted_Class\"] = y_pred\n",
        "for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    val_df[f\"PyMC_Prob_{label}\"] = mean_probs[:, i]\n",
        "val_df.to_csv(\"cgbn_pymc_predictions.csv\", index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4.1 save files\n",
        "import arviz as az\n",
        "\n",
        "# ‚úÖ Save posterior trace\n",
        "az.to_netcdf(trace, \"cgbn_trace.nc\")\n",
        "\n",
        "# ‚úÖ Save summary\n",
        "summary = az.summary(trace, var_names=[\"coeffs\", \"intercept\"])\n",
        "summary.to_csv(\"cgbn_trace_summary.csv\")\n",
        "\n",
        "# ‚úÖ Save predictions\n",
        "val_df[\"PyMC_Predicted_Class\"] = y_pred\n",
        "for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    val_df[f\"PyMC_Prob_{label}\"] = mean_probs[:, i]\n",
        "val_df.to_csv(\"cgbn_pymc_predictions.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Download all saved files from Colab\n",
        "from google.colab import files\n",
        "files.download(\"cgbn_trace.nc\")\n",
        "files.download(\"cgbn_trace_summary.csv\")\n",
        "files.download(\"cgbn_pymc_predictions.csv\")\n",
        "\n",
        "print(\"‚úÖ Files saved and downloading now.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xUVLRmClG--H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fa1113b6-cbf0-40e2-e221-6602a9e2d704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Shape validation failed: input_shape: (1, 1000), minimum_shape: (chains=2, draws=4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6375f778-7a8b-4c9f-8cc0-97ae8ac83705\", \"cgbn_trace.nc\", 501390)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e11e828a-3d8e-40b3-a9c9-7700a3b2b445\", \"cgbn_trace_summary.csv\", 3044)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_619b0810-ed95-4c02-8649-8658597f2eb3\", \"cgbn_pymc_predictions.csv\", 2285607)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Files saved and downloading now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5 Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_val_input, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# ‚úÖ Classification Report and Accuracy\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val_input, y_pred, target_names=[\"Low\", \"Moderate\", \"Severe\"]))\n",
        "\n",
        "print(\"\\nAccuracy Score:\")\n",
        "print(accuracy_score(y_val_input, y_pred))\n",
        "\n",
        "# ‚úÖ Sensitivity (Recall) per class\n",
        "sensitivity = cm.diagonal() / cm.sum(axis=1)\n",
        "print(\"\\nSensitivity (Recall) by Class:\", sensitivity)\n",
        "\n",
        "# ‚úÖ Specificity per class\n",
        "specificity = []\n",
        "for i in range(len(cm)):\n",
        "    true_negatives = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
        "    false_positives = np.sum(np.delete(cm[:, i], i))\n",
        "    specificity.append(true_negatives / (true_negatives + false_positives))\n",
        "print(\"Specificity by Class:\", specificity)\n",
        "\n",
        "# ‚úÖ Macro averages\n",
        "overall_sensitivity = np.mean(sensitivity)\n",
        "overall_specificity = np.mean(specificity)\n",
        "print(\"\\n‚úÖ Overall Sensitivity (Macro Avg):\", overall_sensitivity)\n",
        "print(\"‚úÖ Overall Specificity (Macro Avg):\", overall_specificity)\n",
        "\n",
        "# ‚úÖ Posterior trace plots\n",
        "az.plot_trace(trace, var_names=[\"coeffs\", \"intercept\"])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Trace summary (optional repeat)\n",
        "az.summary(trace, var_names=[\"coeffs\", \"intercept\"])\n",
        "\n",
        "# ‚úÖ Save trace\n",
        "az.to_netcdf(trace, \"cgbn_trace.nc\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kPWP9KvWGs_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Had to change the validation method due to stochasticity caused by the bayesian sampling."
      ],
      "metadata": {
        "id": "QQCDCVWYNyug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5.1 Confidence intervals for the model\n",
        "\n",
        "# Calculate 95% confidence intervals (2.5 and 97.5 percentiles)\n",
        "lower_bound = np.percentile(probs, 2.5, axis=0)\n",
        "upper_bound = np.percentile(probs, 97.5, axis=0)\n",
        "\n",
        "# CI width per sample per class\n",
        "ci_widths = upper_bound - lower_bound\n",
        "\n",
        "# Average CI width by class\n",
        "avg_ci_widths = ci_widths.mean(axis=0)\n",
        "print(\"\\nüîç Average CI Widths:\")\n",
        "for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    print(f\"{label}: {avg_ci_widths[i]:.4f}\")\n",
        "\n",
        "for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    val_df[f\"{label}_CI_Lower\"] = lower_bound[:, i]\n",
        "    val_df[f\"{label}_CI_Upper\"] = upper_bound[:, i]\n",
        "    val_df[f\"{label}_CI_Width\"] = ci_widths[:, i]\n",
        "\n",
        "# Save updated file\n",
        "val_df.to_csv(\"cgbn_pymc_predictions_with_CI.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "wIAEoKwwqkM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc4cfc0-82e1-40ae-d14a-662a26b5ae17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Average CI Widths:\n",
            "Low: 0.0389\n",
            "Moderate: 0.0482\n",
            "Severe: 0.0336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5.2 Add date and time\n",
        "# Load GBN output\n",
        "gbn_df = pd.read_csv(\"cgbn_pymc_predictions.csv\")\n",
        "\n",
        "# Load ensemble metadata (already aligned)\n",
        "ensemble_df = pd.read_csv(\"Full_CI_with_LSTM_XGB_Ensemble.csv\", parse_dates=[\"DATE_and_Time\"])\n",
        "\n",
        "# Add timestamps to GBN\n",
        "gbn_df[\"DATE_and_Time\"] = ensemble_df[\"DATE_and_Time\"]\n"
      ],
      "metadata": {
        "id": "KOGZd26dBTBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5.2.1 Combine with LSTM and XGBoost files while adding date and time.\n",
        "import google.colab.files\n",
        "\n",
        "# Load GBN output\n",
        "gbn_df = pd.read_csv(\"cgbn_pymc_predictions.csv\")\n",
        "\n",
        "# Load ensemble metadata (already aligned)\n",
        "ensemble_df = pd.read_csv(\"Full_CI_with_LSTM_XGB_Ensemble.csv\", parse_dates=[\"DATE_and_Time\"])\n",
        "\n",
        "# Add timestamps to GBN\n",
        "gbn_df[\"DATE_and_Time\"] = ensemble_df[\"DATE_and_Time\"]\n",
        "# probs: shape (n_samples, n_classes, n_post_samples)\n",
        "# Already computed in GBN script: probs\n",
        "\n",
        "lower = np.percentile(probs, 2.5, axis=0)\n",
        "upper = np.percentile(probs, 97.5, axis=0)\n",
        "mean_probs = probs.mean(axis=0)\n",
        "ci_widths = upper - lower\n",
        "\n",
        "# Assemble into CI DataFrame\n",
        "ci_gbn = pd.DataFrame({\n",
        "    \"Mean Prediction (GBN Low)\": mean_probs[:, 0],\n",
        "    \"Lower Bound (GBN Low 95% CI)\": lower[:, 0],\n",
        "    \"Upper Bound (GBN Low 95% CI)\": upper[:, 0],\n",
        "    \"GBN Low CI Width\": ci_widths[:, 0],\n",
        "\n",
        "    \"Mean Prediction (GBN Moderate)\": mean_probs[:, 1],\n",
        "    \"Lower Bound (GBN Moderate 95% CI)\": lower[:, 1],\n",
        "    \"Upper Bound (GBN Moderate 95% CI)\": upper[:, 1],\n",
        "    \"GBN Moderate CI Width\": ci_widths[:, 1],\n",
        "\n",
        "    \"Mean Prediction (GBN Severe)\": mean_probs[:, 2],\n",
        "    \"Lower Bound (GBN Severe 95% CI)\": lower[:, 2],\n",
        "    \"Upper Bound (GBN Severe 95% CI)\": upper[:, 2],\n",
        "    \"GBN Severe CI Width\": ci_widths[:, 2]\n",
        "})\n",
        "# ‚úÖ Load the existing full CI file\n",
        "ci_full = pd.read_csv(\"Full_CI_with_LSTM_XGB_Ensemble.csv\", parse_dates=[\"DATE_and_Time\"])\n",
        "\n",
        "# ‚úÖ Merge on index (make sure lengths match)\n",
        "assert len(ci_gbn) == len(ci_full), \"Length mismatch between GBN and Ensemble CI!\"\n",
        "ci_full = pd.concat([ci_full, ci_gbn.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# ‚úÖ Save updated file\n",
        "ci_full.to_csv(\"Full_CI_with_LSTM_XGB_Ensemble_GBN.csv\", index=False)\n",
        "print(\"‚úÖ Combined CI with GBN saved.\")\n",
        "google.colab.files.download(\"Full_CI_with_LSTM_XGB_Ensemble_GBN.csv\")\n",
        "\n",
        "print(\"\\nüîç Avg CI Widths ‚Äî GBN\")\n",
        "print(\"Low:\", ci_gbn[\"GBN Low CI Width\"].mean())\n",
        "print(\"Moderate:\", ci_gbn[\"GBN Moderate CI Width\"].mean())\n",
        "print(\"Severe:\", ci_gbn[\"GBN Severe CI Width\"].mean())\n"
      ],
      "metadata": {
        "id": "HEY5Rx7truCc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0e198721-c103-429e-db7e-3afb48ccc696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Combined CI with GBN saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2c94bddc-c7e0-4338-8dcb-48d9307ab956\", \"Full_CI_with_LSTM_XGB_Ensemble_GBN.csv\", 5229109)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Avg CI Widths ‚Äî GBN\n",
            "Low: 0.038942013075781025\n",
            "Moderate: 0.04817001952740898\n",
            "Severe: 0.0335588672617576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 6: Visualize Bayesian Model Structure & Coefficient Effects\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import graphviz\n",
        "\n",
        "# ‚úÖ PyMC Computational Graph (Model Structure)\n",
        "print(\"üìå Bayesian Model Structure (PyMC Graph)\")\n",
        "pm.model_to_graphviz(model)  # This will render directly in Colab\n",
        "\n",
        "# ‚úÖ Rename coefficient dimensions for clean forest plot\n",
        "# This adds actual feature names as coordinates for display\n",
        "trace = trace.assign_coords({\"coeffs_dim_0\": feature_names})\n",
        "\n",
        "# ‚úÖ Posterior Coefficient Effects (Forest Plot)\n",
        "# ‚úÖ Plot posterior distributions for coefficients\n",
        "az.plot_forest(\n",
        "    trace,\n",
        "    var_names=[\"coeffs\"],\n",
        "    combined=True,\n",
        "    hdi_prob=0.95,  # replaces credible_interval\n",
        "    ridgeplot_overlap=2,\n",
        "    colors=\"cycle\"\n",
        ")\n",
        "plt.title(\"Posterior Distributions of Feature Coefficients (PyMC CGBN)\", fontsize=14)\n",
        "plt.xlabel(\"Coefficient Value\", fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fENNghX2sm3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ‚úÖ Manual edge list based on temporal + model dependencies\n",
        "edges = [\n",
        "    (\"Wet_Bulb_Lag_2\", \"Risk_Category\"),\n",
        "    (\"Wet_Bulb_Lag_3\", \"Risk_Category\"),\n",
        "    (\"Wind_Speed_Lag_2\", \"Risk_Category\"),\n",
        "    (\"Altimeter_Lag_3\", \"Risk_Category\"),\n",
        "    (\"Wind_Direction_Lag_2\", \"Risk_Category\"),\n",
        "\n",
        "    (\"Risk_Low_Lag_2\", \"Risk_Category\"),\n",
        "    (\"Risk_Low_Lag_24\", \"Risk_Category\"),\n",
        "    (\"Risk_Moderate_Lag_2\", \"Risk_Category\"),\n",
        "    (\"Risk_Moderate_Lag_24\", \"Risk_Category\"),\n",
        "    (\"Risk_Severe_Lag_2\", \"Risk_Category\"),\n",
        "    (\"Risk_Severe_Lag_24\", \"Risk_Category\"),\n",
        "\n",
        "    (\"hour_sin\", \"Risk_Category\"),\n",
        "    (\"hour_cos\", \"Risk_Category\"),\n",
        "    (\"month_sin\", \"Risk_Category\"),\n",
        "    (\"month_cos\", \"Risk_Category\")\n",
        "]\n",
        "\n",
        "# ‚úÖ Create and style the graph\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Node styling\n",
        "node_colors = []\n",
        "for node in G.nodes():\n",
        "    if \"Risk_\" in node and \"Lag\" in node:\n",
        "        node_colors.append(\"orange\")  # Past risk lags\n",
        "    elif \"Wet_Bulb\" in node or \"Wind\" in node or \"Altimeter\" in node:\n",
        "        node_colors.append(\"lightgreen\")  # Meteorological\n",
        "    elif \"hour\" in node or \"month\" in node:\n",
        "        node_colors.append(\"skyblue\")  # Temporal features\n",
        "    elif node == \"Risk_Category\":\n",
        "        node_colors.append(\"red\")  # Target\n",
        "    else:\n",
        "        node_colors.append(\"gray\")\n",
        "\n",
        "# Layout\n",
        "pos = nx.spring_layout(G, k=0.6, seed=42)\n",
        "\n",
        "# Draw\n",
        "plt.figure(figsize=(12, 8))\n",
        "nx.draw(\n",
        "    G,\n",
        "    pos,\n",
        "    with_labels=True,\n",
        "    node_color=node_colors,\n",
        "    node_size=2500,\n",
        "    font_size=10,\n",
        "    font_weight='bold',\n",
        "    edge_color=\"gray\",\n",
        "    arrows=True,\n",
        "    arrowstyle='-|>',\n",
        "    arrowsize=20\n",
        ")\n",
        "plt.title(\"Bayesian Network Structure (Manual, PyMC CGBN)\", fontsize=14)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "818aEHfeWsOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluate GBN Risk Estimates (clean version)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Map encoded risk levels to human-readable labels\n",
        "label_map = {0: \"Low\", 1: \"Moderate\", 2: \"Severe\"}\n",
        "val_df[\"Actual_Risk_Label\"] = val_df[\"Risk_Category_Encoded\"].map(label_map)\n",
        "val_df[\"Predicted_Risk_Label\"] = val_df[\"PyMC_Predicted_Class\"].map(label_map)\n",
        "\n",
        "# ‚úÖ Drop any rows with missing labels (just in case)\n",
        "filtered_df = val_df.dropna(subset=[\"Actual_Risk_Label\", \"Predicted_Risk_Label\"])\n",
        "\n",
        "# ‚úÖ Extract labels\n",
        "y_true = filtered_df[\"Actual_Risk_Label\"]\n",
        "y_pred = filtered_df[\"Predicted_Risk_Label\"]\n",
        "labels = [\"Low\", \"Moderate\", \"Severe\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PTO0u1oqVDJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7.3.1\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"CGBN Risk Prediction Confusion Matrix (Counts)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_dWd_xT9YPhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7.3.2\n",
        "cm_percent = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]*100\n",
        "cm_percent_df = pd.DataFrame(cm_percent, index=labels, columns=labels)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_percent_df, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"CGBN Risk Prediction Confusion Matrix (Percentage)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iXl_fUSOYSnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7.3.3\n",
        "print(\"üìä CGBN Risk Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, labels=labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2jCX8VeYWQ_",
        "outputId": "308d28ba-259b-414f-af45-94c709c81d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä CGBN Risk Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Low       0.67      0.78      0.72      3094\n",
            "    Moderate       0.70      0.53      0.60      4140\n",
            "      Severe       0.51      0.69      0.59      1430\n",
            "\n",
            "    accuracy                           0.64      8664\n",
            "   macro avg       0.63      0.67      0.64      8664\n",
            "weighted avg       0.66      0.64      0.64      8664\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36TggVI23g4M"
      },
      "outputs": [],
      "source": [
        "# Step 8: ROC\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.special\n",
        "\n",
        "# Convert predicted probabilities from mean_probs (shape: [n_samples, 3])\n",
        "y_pred_probs = mean_probs.copy()  # already normalized softmax from trace\n",
        "\n",
        "# Convert true class labels (0 = Low, 1 = Moderate, 2 = Severe)\n",
        "y_true_classes = val_df[\"Risk_Category_Encoded\"].values\n",
        "\n",
        "# One-hot encode true labels for ROC\n",
        "y_true_bin = label_binarize(y_true_classes, classes=[0, 1, 2])\n",
        "\n",
        "# Optional: apply logit scaling to probabilities (can help smooth ROC curves)\n",
        "y_pred_probs_scaled = scipy.special.expit(y_pred_probs * 2)  # Adjust the scale factor if needed\n",
        "\n",
        "# Compute ROC and AUC for each class\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "\n",
        "for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs_scaled[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Macro-average\n",
        "fpr_macro = np.linspace(0, 1, 100)\n",
        "tpr_macro = np.zeros_like(fpr_macro)\n",
        "for i in range(len(fpr)):\n",
        "    tpr_macro += np.interp(fpr_macro, fpr[i], tpr[i])\n",
        "tpr_macro /= len(fpr)\n",
        "roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
        "\n",
        "# Micro-average\n",
        "fpr_micro, tpr_micro, _ = roc_curve(y_true_bin.ravel(), y_pred_probs_scaled.ravel())\n",
        "roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i, label in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    plt.plot(fpr[i], tpr[i], label=f\"{label} Risk (AUC = {roc_auc[i]:.3f})\")\n",
        "\n",
        "plt.plot(fpr_macro, tpr_macro, linestyle='dashed', label=f\"Macro Avg (AUC = {roc_auc_macro:.3f})\")\n",
        "plt.plot(fpr_micro, tpr_micro, linestyle='dotted', label=f\"Micro Avg (AUC = {roc_auc_micro:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"CGBN ROC Curve (With Logit Scaling)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for i, class_name in enumerate([\"Low\", \"Moderate\", \"Severe\"]):\n",
        "    y_true_binary = (y_true_classes == i).astype(int)\n",
        "    prob_true, prob_pred = calibration_curve(y_true_binary, y_pred_probs_scaled[:, i], n_bins=10)\n",
        "    plt.plot(prob_pred, prob_true, marker='o', label=f\"{class_name} Risk\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Perfect Calibration\")\n",
        "\n",
        "plt.xlabel(\"Predicted Probability\")\n",
        "plt.ylabel(\"True Probability\")\n",
        "plt.title(\"CGBN Calibration Curves (With Logit Scaling)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added probability calibration for the BRNN"
      ],
      "metadata": {
        "id": "7nYAipZsTU1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 9.1 confidence interval\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate 95% credible intervals from posterior predictive samples\n",
        "# probs shape: [num_samples, num_timesteps, num_classes]\n",
        "lower_ci = np.percentile(probs, 2.5, axis=0)   # shape: [n_time, n_class]\n",
        "upper_ci = np.percentile(probs, 97.5, axis=0)\n",
        "mean_probs = probs.mean(axis=0)                # already used for y_pred\n",
        "\n",
        "# Plot confidence bands per class\n",
        "classes = [\"Low\", \"Moderate\", \"Severe\"]\n",
        "timesteps = np.arange(mean_probs.shape[0])\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i, label in enumerate(classes):\n",
        "    plt.plot(timesteps, mean_probs[:, i], label=f\"{label} Mean Prob\", linewidth=2)\n",
        "    plt.fill_between(\n",
        "        timesteps,\n",
        "        lower_ci[:, i],\n",
        "        upper_ci[:, i],\n",
        "        alpha=0.3,\n",
        "        label=f\"{label} 95% CI\"\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Predicted Probability\")\n",
        "plt.title(\"CGBN Predicted Probabilities with 95% Confidence Intervals\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Cm8JlqFaYrzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "# Compute entropy per timestep (higher = less confident)\n",
        "uncertainty = entropy(mean_probs.T)  # shape: [n_timesteps]\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.plot(timesteps, uncertainty, color='red', linewidth=2)\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Entropy\")\n",
        "plt.title(\"Prediction Uncertainty (Entropy) Across Validation Timeline\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SBihYo8ucsE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a three tier ensemble with CGBN, LSTM, and XGBoost for IAH SVM SMOTE only."
      ],
      "metadata": {
        "id": "NrjklGP9haMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensemble step 1, load files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load XGBoost predictions and ground truth\n",
        "xgb_df = pd.read_csv(\"val_XGBoost_with_Categorical_Risk_with_date.csv\")\n",
        "\n",
        "# Load LSTM predictions\n",
        "lstm_preds = np.load(\"lstm_preds.npy\")\n",
        "\n",
        "# Load CGBN predictions\n",
        "cgbn_df = pd.read_csv(\"cgbn_pymc_predictions.csv\")  # or cgbn_val_df_with_probs.csv\n"
      ],
      "metadata": {
        "id": "wCsASCk3fUbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"LSTM predictions:      {len(lstm_preds)}\")\n",
        "print(f\"XGBoost predictions:   {len(xgb_df)}\")\n",
        "print(f\"CGBN predictions:      {len(cgbn_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_PPTW2Khrmg",
        "outputId": "1b70d96b-0156-4f81-acea-f7e7bddadd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM predictions:      8664\n",
            "XGBoost predictions:   8664\n",
            "CGBN predictions:      8664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trim XGBoost to match LSTM and CGBN\n",
        "xgb_df = xgb_df.iloc[:8664].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "KfAtc9fjh9LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xgb_df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU6PIQzCihX9",
        "outputId": "c1acc213-b026-4767-f2bd-eb50e424c7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Max_Risk_Quartile', 'Max_Risk_Quartile_Lag_2', 'Max_Risk_Quartile_Lag_24', 'Wet_Bulb_Lag_2', 'Wet_Bulb_Lag_3', 'Wind_Speed_Lag_2', 'Altimeter_Lag_3', 'Wind_Direction_Lag_2', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'Max_Risk', 'Predicted_Max_Risk', 'Actual_Risk', 'Predicted_Risk', 'DATE_and_Time']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_xgb = pd.read_csv(\"val_XGBoost_with_Categorical_Risk_with_date.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eQbooDvLkaLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the numeric predictions directly\n",
        "xgb_probs_trimmed = np.zeros((len(val_xgb), 3))\n",
        "for i, pred in enumerate(val_xgb[\"Predicted_Risk\"]):\n",
        "    xgb_probs_trimmed[i, int(pred)] = 1\n"
      ],
      "metadata": {
        "id": "nQIPo3R0kh4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xgb_probs_trimmed.shape)  # Should be (8760, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d13TRIKck12l",
        "outputId": "00e4a5f1-8dd9-4af3-a92d-4fe5fc41e2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8664, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_probs_trimmed = xgb_probs_trimmed[-8664:]\n"
      ],
      "metadata": {
        "id": "UkDK3XoZk3tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_preds = np.load(\"lstm_preds.npy\")\n",
        "lstm_probs = np.zeros((len(lstm_preds), 3))\n",
        "for i, pred in enumerate(lstm_preds):\n",
        "    lstm_probs[i, int(pred)] = 1\n"
      ],
      "metadata": {
        "id": "SyQlHL8UlSey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load true labels for evaluation\n",
        "y_val_lstm = np.load(\"y_val_LSTM.npy\", allow_pickle=True)\n",
        "y_val_lstm_labels = np.argmax(y_val_lstm, axis=1)  # Convert from one-hot to class indices\n",
        "\n",
        "print(\"‚úÖ Loaded y_val_lstm_labels:\", y_val_lstm_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrgxrNTnlklr",
        "outputId": "5b086f78-a3a9-4ff9-ef1f-cd5b30989905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded y_val_lstm_labels: (8664,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2, Buld the ensemble:\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Confirm probability arrays are aligned\n",
        "# lstm_probs: (8664, 3)\n",
        "# xgb_probs_trimmed: (8664, 3)\n",
        "# mean_probs (CGBN): (8664, 3)\n",
        "\n",
        "# Step 2: Weighted Ensemble Averaging\n",
        "ensemble_probs = (\n",
        "    0.50 * xgb_probs_trimmed +\n",
        "    0.30 * lstm_probs +\n",
        "    0.2 * mean_probs\n",
        ")\n",
        "\n",
        "# Step 3: Convert to class predictions\n",
        "final_preds = np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "# Step 4: True labels from LSTM validation set\n",
        "y_val_lstm_labels = np.argmax(y_val_lstm, axis=1)\n",
        "\n",
        "# Step 5: Evaluation\n",
        "ensemble_accuracy = accuracy_score(y_val_lstm_labels, final_preds)\n",
        "ensemble_cm = confusion_matrix(y_val_lstm_labels, final_preds)\n",
        "ensemble_report = classification_report(y_val_lstm_labels, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "\n",
        "# Step 6: Confusion Matrix Heatmaps\n",
        "cm_df = pd.DataFrame(ensemble_cm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                     columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"CGBN-Integrated Ensemble Confusion Matrix (Counts)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Normalized percentages\n",
        "cm_percent = ensemble_cm.astype(float) / ensemble_cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "cm_percent_df = pd.DataFrame(cm_percent, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                             columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_percent_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.title(\"CGBN-Integrated Ensemble Confusion Matrix (Percentages)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Report\n",
        "print(f\"‚úÖ Ensemble Accuracy (CGBN integrated): {ensemble_accuracy:.4%}\")\n",
        "print(\"‚úÖ Ensemble Classification Report:\\n\", ensemble_report)\n",
        "\n"
      ],
      "metadata": {
        "id": "1CkWoa3dhiUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensemble 2.1, using entropy\n",
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "\n",
        "# mean_probs = CGBN's softmax probability predictions (shape: [8664, 3])\n",
        "cgbn_entropy = entropy(mean_probs.T)  # shape: (8664,)\n",
        "\n",
        "# Normalize entropy to range [0, 1]\n",
        "cgbn_entropy_norm = (cgbn_entropy - cgbn_entropy.min()) / (cgbn_entropy.max() - cgbn_entropy.min())\n",
        "\n",
        "# Invert entropy to get confidence weights: higher confidence ‚Üí higher weight\n",
        "cgbn_confidence_weights = 1 - cgbn_entropy_norm\n"
      ],
      "metadata": {
        "id": "h-d7Cu-bnCdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2 Rebuild probabilities\n",
        "# One-hot LSTM\n",
        "lstm_probs = np.zeros((len(lstm_preds), 3))\n",
        "for i, pred in enumerate(lstm_preds):\n",
        "    lstm_probs[i, int(pred)] = 1\n",
        "\n",
        "# One-hot XGBoost\n",
        "xgb_probs = np.zeros((len(val_xgb), 3))\n",
        "for i, pred in enumerate(val_xgb[\"Predicted_Risk\"]):\n",
        "    xgb_probs[i, int(pred)] = 1\n",
        "\n",
        "# Trim to match LSTM/CGBN if needed\n",
        "xgb_probs = xgb_probs[-8664:]\n"
      ],
      "metadata": {
        "id": "xSaGUO6FnMyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.3 Create entropy weighted ensemble\n",
        "# Fixed weights\n",
        "w_lstm = 0.30\n",
        "w_xgb = 0.50\n",
        "\n",
        "# Final weight for each sample = 1 - fixed weights\n",
        "w_cgbn = 1.0 - (w_lstm + w_xgb)  # Should be 0.20 if w_lstm=0.30, w_xgb=0.50\n",
        "\n",
        "# Apply entropy-based scaling to CGBN\n",
        "weighted_cgbn = (cgbn_confidence_weights * w_cgbn)[:, np.newaxis] * mean_probs\n",
        "\n",
        "# Broadcast constant weights\n",
        "weighted_lstm = w_lstm * lstm_probs\n",
        "weighted_xgb = w_xgb * xgb_probs\n",
        "\n",
        "# Final entropy-weighted ensemble\n",
        "ensemble_probs = weighted_lstm + weighted_xgb + weighted_cgbn\n",
        "final_preds = np.argmax(ensemble_probs, axis=1)\n"
      ],
      "metadata": {
        "id": "OYrlPr3VnPP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# True labels (from LSTM validation)\n",
        "y_val_lstm_labels = np.argmax(y_val_lstm, axis=1)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_val_lstm_labels, final_preds)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_val_lstm_labels, final_preds)\n",
        "cm_df = pd.DataFrame(cm, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                     columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# Report\n",
        "report = classification_report(y_val_lstm_labels, final_preds, target_names=[\"Low\", \"Moderate\", \"Severe\"])\n",
        "\n",
        "# Heatmap (Percent)\n",
        "cm_percent = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "cm_percent_df = pd.DataFrame(cm_percent, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                             columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_percent_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.title(\"Entropy-Weighted Ensemble Confusion Matrix (Percentages)\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final Printout\n",
        "print(f\"‚úÖ Entropy-Weighted Ensemble Accuracy: {accuracy:.4%}\")\n",
        "print(\"‚úÖ Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "5PjDqj6ongV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensemble 3, weighted voting, automated\n",
        "# Use Severe CI width (or Moderate/Low based on class focus)\n",
        "ci_width = ci_results_cgbn[\"Severe CI Width\"].values  # shape (8664,)\n",
        "normalized_ci_width = (ci_width - ci_width.min()) / (ci_width.max() - ci_width.min())\n",
        "\n",
        "# Invert to get confidence weight: narrower CI ‚Üí higher confidence\n",
        "cgbn_conf_weight = 1 - normalized_ci_width  # shape (8664,)\n",
        "cgbn_conf_weight = np.clip(cgbn_conf_weight, 0, 1)  # Safety clamp\n",
        "\n",
        "w_lstm = 0.30\n",
        "w_xgb = 0.50\n",
        "max_w_cgbn = 0.20  # Max weight CGBN can contribute\n",
        "\n",
        "# Broadcast fixed weights\n",
        "weighted_lstm = w_lstm * lstm_probs\n",
        "weighted_xgb = w_xgb * xgb_probs_trimmed\n",
        "\n",
        "# Dynamically scale CGBN per sample\n",
        "weighted_cgbn = (cgbn_conf_weight * max_w_cgbn)[:, np.newaxis] * mean_probs\n",
        "\n",
        "# Combine ensemble\n",
        "ensemble_probs_ci = weighted_lstm + weighted_xgb + weighted_cgbn\n",
        "final_preds_ci = np.argmax(ensemble_probs_ci, axis=1)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Ground truth labels\n",
        "y_true = np.argmax(y_val_lstm, axis=1)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_true, final_preds_ci)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, final_preds_ci)\n",
        "cm_percent = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "cm_percent_df = pd.DataFrame(cm_percent, index=[\"Actual Low\", \"Actual Moderate\", \"Actual Severe\"],\n",
        "                             columns=[\"Pred Low\", \"Pred Moderate\", \"Pred Severe\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_percent_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.title(\"CI-Weighted Ensemble Confusion Matrix (Percentages)\")\n",
        "plt.xlabel(\"Predicted Risk\")\n",
        "plt.ylabel(\"Actual Risk\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Report\n",
        "print(f\"‚úÖ CI-Weighted Ensemble Accuracy: {acc:.4%}\")\n",
        "print(\"‚úÖ Classification Report:\\n\", classification_report(y_true, final_preds_ci, target_names=[\"Low\", \"Moderate\", \"Severe\"]))\n"
      ],
      "metadata": {
        "id": "T1L_WS3zr3p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bootstrap confidence intervals\n",
        "raw_df = pd.read_excel(\"MDW_Data_Python_Imputed.xlsx\", parse_dates=[\"DATE_and_Time\"])\n",
        "val_timestamps = val_xgb[\"DATE_and_Time\"][-min_len:]\n",
        "meta_metadata = raw_df[raw_df[\"DATE_and_Time\"].isin(val_timestamps)].sort_values(\"DATE_and_Time\").reset_index(drop=True)\n",
        "meta_metadata = meta_metadata[-min_len:]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BahM8ndGpRrW",
        "outputId": "f65575e0-e13b-4324-90e0-4d457d60515b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-69-e67fe6584bb6>:4: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
            "  meta_metadata = raw_df[raw_df[\"DATE_and_Time\"].isin(val_timestamps)].sort_values(\"DATE_and_Time\").reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ci_with_meta_cgbn = pd.concat([meta_metadata.reset_index(drop=True), ci_results_cgbn.reset_index(drop=True)], axis=1)\n"
      ],
      "metadata": {
        "id": "N8H_pPgSpoMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload metadata file\n",
        "raw_df = pd.read_excel(\"MDW_Data_Python_Imputed.xlsx\", parse_dates=[\"DATE_and_Time\"])\n",
        "\n",
        "# Use aligned timestamps from val_xgb\n",
        "val_timestamps = val_xgb[\"DATE_and_Time\"][-min_len:]\n",
        "\n",
        "# Filter and align metadata\n",
        "meta_metadata = raw_df[raw_df[\"DATE_and_Time\"].isin(val_timestamps)].sort_values(\"DATE_and_Time\").reset_index(drop=True)\n",
        "meta_metadata = meta_metadata[-min_len:]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEPBxrKvplc2",
        "outputId": "a5a45e6f-fe64-4ad0-c44e-adc874ad6c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-899265e0a9ca>:8: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
            "  meta_metadata = raw_df[raw_df[\"DATE_and_Time\"].isin(val_timestamps)].sort_values(\"DATE_and_Time\").reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step X: Bootstrap Confidence Intervals for CGBN Ensemble (LSTM + XGB + CGBN)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ‚úÖ Weights for the CGBN-integrated ensemble\n",
        "w_lstm = 0.30\n",
        "w_xgb = 0.50\n",
        "w_cgbn = 0.20\n",
        "n_bootstraps = 1000\n",
        "min_len = 8664  # Match alignment with previous ensemble\n",
        "\n",
        "# ‚úÖ Initialize accumulators\n",
        "sum_preds = np.zeros((min_len, 3), dtype=np.float32)\n",
        "sum_preds_sq = np.zeros((min_len, 3), dtype=np.float32)\n",
        "\n",
        "# ‚úÖ Bootstrap loop\n",
        "for i in range(n_bootstraps):\n",
        "    indices = np.random.choice(min_len, size=min_len, replace=True)\n",
        "    sampled_lstm = lstm_probs[indices]\n",
        "    sampled_xgb = xgb_probs_trimmed[indices]\n",
        "    sampled_cgbn = mean_probs[indices]\n",
        "\n",
        "    preds_ensemble = (\n",
        "        w_lstm * sampled_lstm +\n",
        "        w_xgb * sampled_xgb +\n",
        "        w_cgbn * sampled_cgbn\n",
        "    )\n",
        "\n",
        "    sum_preds += preds_ensemble\n",
        "    sum_preds_sq += preds_ensemble**2\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"‚úÖ Bootstrap iteration {i}/{n_bootstraps}\")\n",
        "\n",
        "# ‚úÖ Compute mean, std, and 95% CI bounds\n",
        "mean_preds = sum_preds / n_bootstraps\n",
        "std_preds = np.sqrt((sum_preds_sq / n_bootstraps) - (mean_preds**2))\n",
        "\n",
        "lower_bound = np.clip(mean_preds - 1.96 * std_preds, 0, 1)\n",
        "upper_bound = np.clip(mean_preds + 1.96 * std_preds, 0, 1)\n",
        "ci_widths = upper_bound - lower_bound\n",
        "\n",
        "# ‚úÖ Create CI result DataFrame\n",
        "ci_results_cgbn = pd.DataFrame({\n",
        "    \"Mean Prediction (Low Risk)\": mean_preds[:, 0],\n",
        "    \"Lower Bound (Low 95% CI)\": lower_bound[:, 0],\n",
        "    \"Upper Bound (Low 95% CI)\": upper_bound[:, 0],\n",
        "    \"Low CI Width\": ci_widths[:, 0],\n",
        "\n",
        "    \"Mean Prediction (Moderate Risk)\": mean_preds[:, 1],\n",
        "    \"Lower Bound (Moderate 95% CI)\": lower_bound[:, 1],\n",
        "    \"Upper Bound (Moderate 95% CI)\": upper_bound[:, 1],\n",
        "    \"Moderate CI Width\": ci_widths[:, 1],\n",
        "\n",
        "    \"Mean Prediction (Severe Risk)\": mean_preds[:, 2],\n",
        "    \"Lower Bound (Severe 95% CI)\": lower_bound[:, 2],\n",
        "    \"Upper Bound (Severe 95% CI)\": upper_bound[:, 2],\n",
        "    \"Severe CI Width\": ci_widths[:, 2]\n",
        "})\n",
        "\n",
        "# ‚úÖ Merge with metadata (from your earlier load)\n",
        "ci_with_meta_cgbn = pd.concat([meta_metadata.reset_index(drop=True), ci_results_cgbn.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# ‚úÖ Save result\n",
        "save_path = \"CGBN_Ensemble_Bootstrapped_CI_with_Metadata.csv\"\n",
        "ci_with_meta_cgbn.to_csv(save_path, index=False)\n",
        "print(f\"\\n‚úÖ Saved CGBN Ensemble CI + Metadata to: {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ky8CeRtoyAw",
        "outputId": "b4349e9d-924c-449d-d53c-6c7f89f5c9b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Bootstrap iteration 0/1000\n",
            "‚úÖ Bootstrap iteration 100/1000\n",
            "‚úÖ Bootstrap iteration 200/1000\n",
            "‚úÖ Bootstrap iteration 300/1000\n",
            "‚úÖ Bootstrap iteration 400/1000\n",
            "‚úÖ Bootstrap iteration 500/1000\n",
            "‚úÖ Bootstrap iteration 600/1000\n",
            "‚úÖ Bootstrap iteration 700/1000\n",
            "‚úÖ Bootstrap iteration 800/1000\n",
            "‚úÖ Bootstrap iteration 900/1000\n",
            "\n",
            "‚úÖ Saved CGBN Ensemble CI + Metadata to: CGBN_Ensemble_Bootstrapped_CI_with_Metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('CGBN_Ensemble_Bootstrapped_CI_with_Metadata.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_EDrDBkJqNRg",
        "outputId": "0a92aec2-bc81-45b6-f8a0-b3e19dd44471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fa85ef16-a067-4c38-8713-69f55d3ae6a1\", \"CGBN_Ensemble_Bootstrapped_CI_with_Metadata.csv\", 1540105)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(mean_preds[:, 2], label=\"Severe Risk (Mean)\", color=\"red\")\n",
        "plt.fill_between(range(min_len), lower_bound[:, 2], upper_bound[:, 2], alpha=0.2, color=\"red\")\n",
        "plt.title(\"Bootstrapped Confidence Interval - Severe Risk (CGBN Ensemble)\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Predicted Probability\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "07Gva2QepCww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparison from adding ensemble\n",
        "# Load both CI results if not in memory\n",
        "ci1 = pd.read_csv(\"Ensemble2_Bootstrapped_CI_with_Metadata.csv\")\n",
        "ci2 = pd.read_csv(\"CGBN_Ensemble_Bootstrapped_CI_with_Metadata.csv\")\n",
        "\n",
        "# Compare median CI width by class\n",
        "print(\"üîç Median CI Width Comparison:\")\n",
        "print(\"Low:     \", ci1[\"Low CI Width\"].median(), \"‚Üí\", ci2[\"Low CI Width\"].median())\n",
        "print(\"Moderate:\", ci1[\"Moderate CI Width\"].median(), \"‚Üí\", ci2[\"Moderate CI Width\"].median())\n",
        "print(\"Severe:  \", ci1[\"Severe CI Width\"].median(), \"‚Üí\", ci2[\"Severe CI Width\"].median())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WBxQY4lqsRF",
        "outputId": "02a5a220-c928-4a96-e552-daf897be7059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Median CI Width Comparison:\n",
            "Low:      1.0 ‚Üí 1.0\n",
            "Moderate: 1.0 ‚Üí 1.0\n",
            "Severe:   0.90452519 ‚Üí 0.85005603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beb4cb51"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOJiu7aIi56TOuU0l7CK7oz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce777994d7db4fa09a6b339c346d41fb": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ad929a87e2f04010bfdff549bdb99e9e",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Fitting: \u001b[38;2;23;100;244m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;2;23;100;244m‚ï∏\u001b[0m \u001b[35m100%\u001b[0m 0:00:01 Average Loss = 38,495\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fitting: <span style=\"color: #1764f4; text-decoration-color: #1764f4\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> 0:00:01 Average Loss = 38,495\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "ad929a87e2f04010bfdff549bdb99e9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}